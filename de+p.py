import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import warnings
import os
from PIL import Image
import cv2
from collections import Counter
from ultralytics import YOLO

# –û—Ç–∫–ª—é—á–∞–µ–º warnings
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
IMG_SIZE = (224, 224)
BATCH_SIZE = 256
NUM_EPOCHS = 50
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")

class TreeDataset(Dataset):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞—Å—Ç–µ–Ω–∏–π —Å –Ω–∞–¥–µ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    def __init__(self, image_paths, labels, plant_types=None, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.plant_types = plant_types
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        plant_type = self.plant_types[idx] if self.plant_types is not None else 0
        
        image = self.load_image_safe(img_path)
        
        if self.transform:
            image = self.transform(image)
        
        return image, label, plant_type
    
    def load_image_safe(self, img_path):
        try:
            image = cv2.imread(str(img_path))
            if image is not None:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image = Image.fromarray(image)
                return image
        except:
            pass
        
        try:
            image = Image.open(img_path).convert('RGB')
            return image
        except:
            pass
        
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {img_path}, —Å–æ–∑–¥–∞–µ–º —á–µ—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
        return Image.new('RGB', IMG_SIZE, color='black')

def load_tree_species_data_separated(porody_folder_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ –¥–µ—Ä–µ–≤—å—è –∏ –∫—É—Å—Ç—ã"""
    porody_path = Path(porody_folder_path)
    
    print(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {porody_path.absolute()}")
    
    if not porody_path.exists():
        print(f"–ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {porody_path}")
        return [], [], [], [], [], []
    
    # –ò—â–µ–º CSV —Ñ–∞–π–ª
    csv_path = porody_path / "labels" / "labels.csv"
    if not csv_path.exists():
        csv_path = porody_path / "labels.csv"
        if not csv_path.exists():
            print("CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return [], [], [], [], [], []
    
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
        print(f"CSV –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return [], [], [], [], [], []
    
    # –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    images_dir = porody_path / "images"
    if not images_dir.exists():
        print("–ü–∞–ø–∫–∞ 'images' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return [], [], [], [], [], []
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –¥–µ—Ä–µ–≤—å—è –∏ –∫—É—Å—Ç—ã
    tree_images = []
    tree_labels = []
    tree_species = []
    
    bush_images = []
    bush_labels = [] 
    bush_species = []
    
    successful = 0
    for _, row in df.iterrows():
        try:
            filename = str(row['filename']).strip()
            img_path = images_dir / filename
            
            if not img_path.exists():
                print(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}")
                continue
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–∞—Å—Ç–µ–Ω–∏—è (0-–¥–µ—Ä–µ–≤–æ, 1-–∫—É—Å—Ç)
            if 'plant_type' in df.columns:
                plant_type = int(row['plant_type'])
            else:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é, –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç
                species_name = str(row['species_name']).lower()
                if any(word in species_name for word in ['—Ä–æ–∑–∞', '–∫—É—Å—Ç', '–∫—É—Å—Ç–∞—Ä–Ω–∏–∫', '–º–æ–∂–∂–µ–≤–µ–ª—å–Ω–∏–∫', '—Å–∏—Ä–µ–Ω—å']):
                    plant_type = 1
                else:
                    plant_type = 0
            
            if plant_type == 0:  # –î–µ—Ä–µ–≤–æ
                tree_images.append(str(img_path))
                tree_labels.append(int(row['species_label']))
                tree_species.append(str(row['species_name']))
            else:  # –ö—É—Å—Ç
                bush_images.append(str(img_path))
                bush_labels.append(int(row['species_label']))
                bush_species.append(str(row['species_name']))
                
            successful += 1
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏: {e}")
            continue
    
    print(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {successful}/{len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"–î–µ—Ä–µ–≤—å—è: {len(tree_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(tree_labels))} –≤–∏–¥–æ–≤")
    print(f"–ö—É—Å—Ç—ã: {len(bush_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(bush_labels))} –≤–∏–¥–æ–≤")
    
    return tree_images, tree_labels, tree_species, bush_images, bush_labels, bush_species

def get_enhanced_transforms():
    train_transform = transforms.Compose([
        transforms.Resize(IMG_SIZE),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.2),
        transforms.RandomRotation(degrees=30),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2)),
        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize(IMG_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

def create_improved_model(num_classes):
    class ImprovedCNN(nn.Module):
        def __init__(self, num_classes):
            super(ImprovedCNN, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                nn.Conv2d(128, 256, kernel_size=3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                nn.Conv2d(256, 512, kernel_size=3, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((1, 1))
            )
            self.classifier = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.BatchNorm1d(256),
                nn.Dropout(0.3),
                nn.Linear(256, 128),
                nn.ReLU(inplace=True),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = x.view(x.size(0), -1)
            x = self.classifier(x)
            return x
    
    return ImprovedCNN(num_classes)

def get_class_weights(labels, num_classes):
    class_counts = Counter(labels)
    print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {dict(class_counts)}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å—Ç–æ—Ç–µ –∫–ª–∞—Å—Å–æ–≤
    weights = []
    for i in range(num_classes):
        if i in class_counts:
            weights.append(1.0 / class_counts[i])
        else:
            weights.append(1.0)  # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∞ –Ω–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
    weights = np.array(weights)
    weights = weights / weights.sum() * len(weights)
    weights = torch.FloatTensor(weights).to(DEVICE)
    
    print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {weights.cpu().numpy()}")
    return weights

def check_class_balance(labels, class_names, dataset_name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
    label_counts = Counter(labels)
    
    print(f"\n–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í ({dataset_name}):")
    total_samples = len(labels)
    for label, count in label_counts.items():
        class_name = class_names[label] if label < len(class_names) else f"Class {label}"
        percentage = (count / total_samples) * 100
        print(f"  {class_name}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ({percentage:.1f}%)")

def train_single_model(model, train_loader, val_loader, num_epochs, num_classes, model_type="plant"):

    model = model.to(DEVICE)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    train_labels = []
    for _, labels, _ in train_loader:
        train_labels.extend(labels.cpu().numpy())
    
    class_weights = get_class_weights(train_labels, num_classes)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
    
    train_losses = []
    val_accuracies = []
    learning_rates = []
    
    best_accuracy = 0
    best_model_state = None
    patience = 10
    patience_counter = 0
    
    for epoch in range(num_epochs):
        try:
            # –û–±—É—á–µ–Ω–∏–µ
            model.train()
            running_loss = 0.0
            batch_count = 0
            
            for batch_idx, (images, labels, _) in enumerate(train_loader):
                try:
                    images = images.to(DEVICE)
                    labels = labels.to(DEVICE)
                    
                    optimizer.zero_grad()
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    
                    running_loss += loss.item()
                    batch_count += 1
                    
                    if batch_idx % 10 == 0:
                        current_lr = optimizer.param_groups[0]['lr']
                        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}, LR: {current_lr:.2e}')
                        
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_idx}: {e}")
                    continue
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            model.eval()
            correct = 0
            total = 0
            val_loss = 0.0
            
            with torch.no_grad():
                for images, labels, _ in val_loader:
                    try:
                        images = images.to(DEVICE)
                        labels = labels.to(DEVICE)
                        outputs = model(images)
                        loss = criterion(outputs, labels)
                        val_loss += loss.item()
                        
                        _, predicted = torch.max(outputs.data, 1)
                        total += labels.size(0)
                        correct += (predicted == labels).sum().item()
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                        continue
            
            accuracy = 100 * correct / total if total > 0 else 0
            avg_train_loss = running_loss / batch_count if batch_count > 0 else 0
            avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
            
            current_lr = optimizer.param_groups[0]['lr']
            learning_rates.append(current_lr)
            train_losses.append(avg_train_loss)
            val_accuracies.append(accuracy)
            
            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%, LR: {current_lr:.2e}')
            

            scheduler.step(accuracy)
            
            # Early stopping –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model_state = model.state_dict().copy()
                patience_counter = 0
                print(f"–ù–æ–≤—ã–π —Ä–µ–∫–æ—Ä–¥ —Ç–æ—á–Ω–æ—Å—Ç–∏: {accuracy:.2f}%")
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
            
        except Exception as e:
            print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —ç–ø–æ—Ö–µ {epoch+1}: {e}")
            continue
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º weights –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {best_accuracy:.2f}%")
    
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    return model, {'train_loss': train_losses, 'val_accuracy': val_accuracies, 'learning_rates': learning_rates}

def can_use_stratified_split(labels, test_size=0.2):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"""
    label_counts = Counter(labels)
    min_samples_per_class = min(label_counts.values())
    
    # –î–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –æ–±—Ä–∞–∑—Ü–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Å–µ
    return min_samples_per_class >= 2 and all(count >= int(1/test_size) + 1 for count in label_counts.values())

def apply_data_augmentation_balance(image_paths, labels, max_samples_per_class=100):
    """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤"""
    label_counts = Counter(labels)
    max_count = max(label_counts.values())
    
    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if max_count <= min(label_counts.values()) * 2:
        return image_paths, labels
    
    print("üîÑ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é...")
    
    augmented_images = list(image_paths)
    augmented_labels = list(labels)
    
    for class_label, count in label_counts.items():
        if count < max_count:
            # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞
            class_indices = [i for i, label in enumerate(labels) if label == class_label]
            needed_samples = min(max_count - count, max_samples_per_class - count)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è)
            for i in range(needed_samples):
                original_idx = class_indices[i % len(class_indices)]
                augmented_images.append(image_paths[original_idx])
                augmented_labels.append(class_label)
    
    print(f"–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {len(augmented_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    return augmented_images, augmented_labels

def prepare_and_train_model(image_paths, labels, species_names, model_type="plant"):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    if len(image_paths) == 0:
        print(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_type}")
        return None, None, []
    
    print(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(labels))} –∫–ª–∞—Å—Å–æ–≤")
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –º–µ—Ç–∫–∏ –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0..C-1
    unique_labels_sorted = sorted(set(labels))
    original_to_compact = {orig: idx for idx, orig in enumerate(unique_labels_sorted)}
    labels_mapped_all = [original_to_compact[l] for l in labels]

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤
    class_names = []
    for orig_label in unique_labels_sorted:
        for path_i, lab in enumerate(labels):
            if lab == orig_label:
                class_names.append(species_names[path_i])
                break

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    check_class_balance(labels_mapped_all, class_names, f"{model_type} (–∏—Å—Ö–æ–¥–Ω—ã–µ)")
    
    # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    balanced_paths, balanced_labels = apply_data_augmentation_balance(image_paths, labels_mapped_all)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    if len(balanced_paths) <= 3:
        print("–û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_paths, train_labels = balanced_paths, balanced_labels
        val_paths, val_labels = balanced_paths, balanced_labels
    elif can_use_stratified_split(balanced_labels):
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ")
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            balanced_paths, balanced_labels, test_size=0.2, random_state=42, stratify=balanced_labels
        )
    else:
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ (—Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞)")
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            balanced_paths, balanced_labels, test_size=0.2, random_state=42, stratify=None
        )
    
    print(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_paths)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {len(val_paths)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    check_class_balance(train_labels, class_names, f"{model_type} (—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ)")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã
    train_transform, val_transform = get_enhanced_transforms()
    
    train_dataset = TreeDataset(train_paths, train_labels, transform=train_transform)
    val_dataset = TreeDataset(val_paths, val_labels, transform=val_transform)
    
    batch_size = min(BATCH_SIZE, len(train_paths))
    if batch_size == 0:
        batch_size = 1
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
    
    num_classes = len(unique_labels_sorted)
    model = create_improved_model(num_classes)
    
    print(f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ {model_type}: {num_classes} –∫–ª–∞—Å—Å–æ–≤")
    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    model, history = train_single_model(model, train_loader, val_loader, 
                                      NUM_EPOCHS, num_classes, model_type)

    return model, history, class_names

def train_separated_models(porody_folder_path):
    """–û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤"""
    print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –° –†–ê–ó–î–ï–õ–ï–ù–ò–ï–ú –ù–ê –î–ï–†–ï–í–¨–Ø –ò –ö–£–°–¢–´")
    print("=" * 60)
    
    (tree_images, tree_labels, tree_species,
     bush_images, bush_labels, bush_species) = load_tree_species_data_separated(porody_folder_path)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤
    tree_model, tree_history, tree_class_names = None, None, []
    if len(tree_images) > 0:
        print("\n" + "="*50)
        print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –î–ï–†–ï–í–¨–ï–í")
        print("="*50)
        tree_model, tree_history, tree_class_names = prepare_and_train_model(
            tree_images, tree_labels, tree_species, "–¥–µ—Ä–µ–≤—å—è"
        )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∫—É—Å—Ç–æ–≤  
    bush_model, bush_history, bush_class_names = None, None, []
    if len(bush_images) > 0:
        print("\n" + "="*50)
        print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ö–£–°–¢–û–í")
        print("="*50)
        bush_model, bush_history, bush_class_names = prepare_and_train_model(
            bush_images, bush_labels, bush_species, "–∫—É—Å—Ç—ã"
        )
    
    return tree_model, bush_model, tree_class_names, bush_class_names

def filter_small_boxes(boxes, image_shape, min_area_percent=0.001, min_side_percent=0.01):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –±–æ–∫—Å—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º –æ—Ç –ø–ª–æ—â–∞–¥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if len(boxes) == 0:
        return boxes
    
    img_height, img_width = image_shape[:2]
    total_image_area = img_width * img_height
    
    min_area = total_image_area * min_area_percent
    min_side = min(img_width, img_height) * min_side_percent
    
    filtered_boxes = []
    
    for box in boxes:
        x1, y1, x2, y2, conf, cls = box
        width = x2 - x1
        height = y2 - y1
        area = width * height
        
        if (area >= min_area and width >= min_side and height >= min_side):
            filtered_boxes.append(box)
    
    return np.array(filtered_boxes)

def advanced_merge_boxes(boxes, size_weight=0.7, conf_weight=0.3, distance_threshold=100):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–æ–∫—Å—ã —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π"""
    if len(boxes) == 0:
        return []
    
    centers = []
    sizes = []
    for box in boxes:
        x1, y1, x2, y2, conf, cls = box
        center_x = (x1 + x2) / 2
        center_y = (y1 + y2) / 2
        width = x2 - x1
        height = y2 - y1
        centers.append([center_x, center_y])
        sizes.append(width * height)
    
    centers = np.array(centers)
    sizes = np.array(sizes)
    
    n_boxes = len(boxes)
    visited = [False] * n_boxes
    clusters = []
    
    for i in range(n_boxes):
        if visited[i]:
            continue
            
        cluster = [i]
        visited[i] = True
        
        for j in range(i+1, n_boxes):
            if visited[j]:
                continue
                
            distance = np.sqrt(((centers[i] - centers[j]) ** 2).sum())
            
            if distance < distance_threshold:
                cluster.append(j)
                visited[j] = True
        
        clusters.append(cluster)
    
    merged_boxes = []
    
    for cluster in clusters:
        cluster_boxes = boxes[cluster]
        cluster_sizes = sizes[cluster]
        
        if len(cluster_boxes) == 1:
            merged_boxes.append(cluster_boxes[0])
        else:
            best_score = -1
            best_box = None
            
            for i, box in enumerate(cluster_boxes):
                x1, y1, x2, y2, conf, cls = box
                size_score = cluster_sizes[i] / max(sizes) if max(sizes) > 0 else 0
                conf_score = conf
                
                total_score = size_weight * size_score + conf_weight * conf_score
                
                if total_score > best_score:
                    best_score = total_score
                    best_box = box
            
            merged_boxes.append(best_box)
    
    return np.array(merged_boxes)

def detect_and_classify_plants(image_path, detection_model, tree_model, bush_model, 
                              tree_class_names, bush_class_names, 
                              min_confidence=0.3, min_area_percent=0.01):
    """–î–µ—Ç–µ–∫—Ü–∏—è —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø–æ —Ç–∏–ø—É —Ä–∞—Å—Ç–µ–Ω–∏—è"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = cv2.imread(image_path)
    if img is None:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        return None, []
    
    print(f"üîç –î–µ—Ç–µ–∫—Ü–∏—è —Ä–∞—Å—Ç–µ–Ω–∏–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏: {Path(image_path).name}")
    
    # –î–µ—Ç–µ–∫—Ü–∏—è —Å –ø–æ–º–æ—â—å—é YOLO
    try:
        results = detection_model.predict(img, conf=min_confidence)
        boxes = results[0].boxes.data.cpu().numpy()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
        return None, []
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–æ–∫—Å–æ–≤
    filtered_boxes = filter_small_boxes(boxes, img.shape, 
                                      min_area_percent=min_area_percent, 
                                      min_side_percent=0.1)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –±–æ–∫—Å–æ–≤
    merged_boxes = advanced_merge_boxes(filtered_boxes, size_weight=0.8, conf_weight=0.2)
    
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞—Å—Ç–µ–Ω–∏–π: {len(merged_boxes)}")
    
    if len(merged_boxes) == 0:
        print("–†–∞—Å—Ç–µ–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
        return img, []
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    transform = transforms.Compose([
        transforms.Resize(IMG_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    classification_results = []
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Ç–µ–Ω–∏—è
    for i, box in enumerate(merged_boxes):
        x1, y1, x2, y2, conf, detection_class = box
        
        # detection_class: 0 - –¥–µ—Ä–µ–≤–æ, 1 - –∫—É—Å—Ç
        plant_type = "–¥–µ—Ä–µ–≤–æ" if detection_class == 0 else "–∫—É—Å—Ç"
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –∏ –∫–ª–∞—Å—Å—ã
        if detection_class == 0 and tree_model is not None:  # –î–µ—Ä–µ–≤–æ
            classification_model = tree_model
            class_names = tree_class_names
            model_type = "–¥–µ—Ä–µ–≤—å–µ–≤"
        elif detection_class == 1 and bush_model is not None:  # –ö—É—Å—Ç
            classification_model = bush_model  
            class_names = bush_class_names
            model_type = "–∫—É—Å—Ç–æ–≤"
        else:
            print(f"–î–ª—è {plant_type} –Ω–µ—Ç –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ—Ç–µ–∫—Ü–∏–∏ –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            classification_results.append({
                'box': (int(x1), int(y1), int(x2), int(y2)),
                'plant_type': plant_type,
                'detection_confidence': conf,
                'species': "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                'classification_confidence': 0.0,
                'species_id': -1,
                'detection_class': detection_class
            })
            continue
        
        # –í—ã—Ä–µ–∑–∞–µ–º –æ–±–ª–∞—Å—Ç—å —Å —Ä–∞—Å—Ç–µ–Ω–∏–µ–º
        padding = 5
        x1_padded = max(0, int(x1) - padding)
        y1_padded = max(0, int(y1) - padding)
        x2_padded = min(img.shape[1], int(x2) + padding)
        y2_padded = min(img.shape[0], int(y2) + padding)
        
        plant_roi = img[y1_padded:y2_padded, x1_padded:x2_padded]
        
        if plant_roi.size == 0:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–µ–∑–∞—Ç—å –æ–±–ª–∞—Å—Ç—å –¥–ª—è {plant_type} {i+1}")
            continue
            
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ PIL Image –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
        try:
            plant_roi_rgb = cv2.cvtColor(plant_roi, cv2.COLOR_BGR2RGB)
            plant_pil = Image.fromarray(plant_roi_rgb)
            
            image_tensor = transform(plant_pil).unsqueeze(0).to(DEVICE)
            
            classification_model.eval()
            with torch.no_grad():
                outputs = classification_model(image_tensor)
                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                confidence, predicted_class = torch.max(probabilities, 0)
            
            predicted_class = predicted_class.item()
            confidence = confidence.item()
            
            species_name = class_names[predicted_class] if predicted_class < len(class_names) else f"Class {predicted_class}"
            
            classification_results.append({
                'box': (int(x1), int(y1), int(x2), int(y2)),
                'plant_type': plant_type,
                'detection_confidence': conf,
                'species': species_name,
                'classification_confidence': confidence,
                'species_id': predicted_class,
                'detection_class': detection_class
            })
            
            print(f"–†–∞—Å—Ç–µ–Ω–∏–µ {i+1} ({plant_type}): {species_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%})")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ {plant_type} {i+1}: {e}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ—Ç–µ–∫—Ü–∏–∏ –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            classification_results.append({
                'box': (int(x1), int(y1), int(x2), int(y2)),
                'plant_type': plant_type,
                'detection_confidence': conf,
                'species': "–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
                'classification_confidence': 0.0,
                'species_id': -1,
                'detection_class': detection_class
            })
            continue
    
    return img, classification_results

def visualize_detection_with_classification(image, boxes, classification_results):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –æ–∫–Ω–æ–º"""
    img_display = image.copy()
    
    for i, (box, result) in enumerate(zip(boxes, classification_results)):
        x1, y1, x2, y2 = result['box']
        plant_type = result['plant_type']
        species = result['species']
        det_conf = result['detection_confidence']
        cls_conf = result['classification_confidence']
        
        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤
        if result['detection_class'] == 0:  # –î–µ—Ä–µ–≤–æ
            color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π
        else:  # –ö—É—Å—Ç
            color = (0, 165, 255)  # –û—Ä–∞–Ω–∂–µ–≤—ã–π
        
        # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
        cv2.rectangle(img_display, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
        
        # –ü–æ–¥–ø–∏—Å—å —Å –≤–∏–¥–æ–º —Ä–∞—Å—Ç–µ–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        label = f"{plant_type}: {species}"
        confidence_label = f"det({det_conf:.2f}), cls({cls_conf:.2f})"
        
        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
        confidence_size = cv2.getTextSize(confidence_label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        
        total_height = label_size[1] + confidence_size[1] + 15
        
        # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        cv2.rectangle(img_display, 
                     (int(x1), int(y1) - total_height),
                     (int(x1) + max(label_size[0], confidence_size[0]), int(y1)),
                     color, -1)
        
        # –¢–µ–∫—Å—Ç
        cv2.putText(img_display, label, 
                   (int(x1), int(y1) - confidence_size[1] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.putText(img_display, confidence_label,
                   (int(x1), int(y1) - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # –ù–æ–º–µ—Ä —Ä–∞—Å—Ç–µ–Ω–∏—è
        cv2.putText(img_display, f"#{i+1}", 
                   (int(x1), int(y1) - total_height - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
    
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    height, width = img_display.shape[:2]
    max_display_size = 1200
    if max(height, width) > max_display_size:
        scale = max_display_size / max(height, width)
        new_width = int(width * scale)
        new_height = int(height * scale)
        img_display = cv2.resize(img_display, (new_width, new_height))
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∑–∞–∫—Ä—ã—Ç–∏—è
    window_name = "–î–µ—Ç–µ–∫—Ü–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞—Å—Ç–µ–Ω–∏–π (–Ω–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)"
    cv2.imshow(window_name, img_display)
    print("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞–Ω–æ. –ù–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –≤ –æ–∫–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
    
    # –ñ–¥–µ–º –Ω–∞–∂–∞—Ç–∏—è –∫–ª–∞–≤–∏—à–∏ (0 - –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ)
    cv2.waitKey(0)
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –æ–∫–Ω–∞ OpenCV
    cv2.destroyAllWindows()
    
    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –æ–∫–Ω–∞ —Ç–æ—á–Ω–æ –∑–∞–∫—Ä—ã–ª–∏—Å—å
    import time
    time.sleep(0.5)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    output_path = "detection_classification_result.jpg"
    cv2.imwrite(output_path, cv2.cvtColor(img_display, cv2.COLOR_RGB2BGR))
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {output_path}")
    
    return img_display

def load_separated_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤"""
    tree_model, bush_model = None, None
    tree_class_names, bush_class_names = [], []
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–µ—Ä–µ–≤—å–µ–≤
        checkpoint = torch.load('model_trees.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        tree_model = create_improved_model(num_classes)
        tree_model.load_state_dict(checkpoint['model_state_dict'])
        tree_class_names = checkpoint['class_names']
        print("–ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"–ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫—É—Å—Ç–æ–≤
        checkpoint = torch.load('model_bushes.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        bush_model = create_improved_model(num_classes)
        bush_model.load_state_dict(checkpoint['model_state_dict'])
        bush_class_names = checkpoint['class_names']
        print("–ú–æ–¥–µ–ª—å –∫—É—Å—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"–ú–æ–¥–µ–ª—å –∫—É—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    return tree_model, bush_model, tree_class_names, bush_class_names

def detect_and_classify_complete(image_path, detection_model, tree_model, bush_model, defects_model,
                               tree_class_names, bush_class_names, defects_class_names,
                               min_confidence=0.3, min_area_percent=0.01):
    """–ü–æ–ª–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –ø–æ—Ä–æ–¥—ã + –¥–µ—Ñ–µ–∫—Ç—ã"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = cv2.imread(image_path)
    if img is None:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        return None, []
    
    print(f"üîç –î–µ—Ç–µ–∫—Ü–∏—è –∏ –ø–æ–ª–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {Path(image_path).name}")
    
    # –î–µ—Ç–µ–∫—Ü–∏—è —Å –ø–æ–º–æ—â—å—é YOLO
    try:
        results = detection_model.predict(img, conf=min_confidence)
        boxes = results[0].boxes.data.cpu().numpy()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
        return None, []
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–æ–∫—Å–æ–≤
    filtered_boxes = filter_small_boxes(boxes, img.shape, 
                                      min_area_percent=min_area_percent, 
                                      min_side_percent=0.1)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –±–æ–∫—Å–æ–≤
    merged_boxes = advanced_merge_boxes(filtered_boxes, size_weight=0.8, conf_weight=0.2)
    
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞—Å—Ç–µ–Ω–∏–π: {len(merged_boxes)}")
    
    if len(merged_boxes) == 0:
        print("–†–∞—Å—Ç–µ–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
        return img, []
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    transform = transforms.Compose([
        transforms.Resize(IMG_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    classification_results = []
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Ç–µ–Ω–∏—è
    for i, box in enumerate(merged_boxes):
        x1, y1, x2, y2, conf, detection_class = box
        
        # detection_class: 0 - –¥–µ—Ä–µ–≤–æ, 1 - –∫—É—Å—Ç
        plant_type = "–¥–µ—Ä–µ–≤–æ" if detection_class == 0 else "–∫—É—Å—Ç"
        
        # –í–´–ë–û–† –ú–û–î–ï–õ–ò –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ü–û–†–û–î–´
        species_model = None
        species_class_names = []
        if detection_class == 0 and tree_model is not None:  # –î–µ—Ä–µ–≤–æ
            species_model = tree_model
            species_class_names = tree_class_names
        elif detection_class == 1 and bush_model is not None:  # –ö—É—Å—Ç
            species_model = bush_model  
            species_class_names = bush_class_names
        
        # –í—ã—Ä–µ–∑–∞–µ–º –æ–±–ª–∞—Å—Ç—å —Å —Ä–∞—Å—Ç–µ–Ω–∏–µ–º
        padding = 5
        x1_padded = max(0, int(x1) - padding)
        y1_padded = max(0, int(y1) - padding)
        x2_padded = min(img.shape[1], int(x2) + padding)
        y2_padded = min(img.shape[0], int(y2) + padding)
        
        plant_roi = img[y1_padded:y2_padded, x1_padded:x2_padded]
        
        if plant_roi.size == 0:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–µ–∑–∞—Ç—å –æ–±–ª–∞—Å—Ç—å –¥–ª—è —Ä–∞—Å—Ç–µ–Ω–∏—è {i+1}")
            continue
        
        species_name = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        species_confidence = 0.0
        defects_name = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        defects_confidence = 0.0
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–û–†–û–î–´
        if species_model is not None:
            try:
                plant_roi_rgb = cv2.cvtColor(plant_roi, cv2.COLOR_BGR2RGB)
                plant_pil = Image.fromarray(plant_roi_rgb)
                
                image_tensor = transform(plant_pil).unsqueeze(0).to(DEVICE)
                
                species_model.eval()
                with torch.no_grad():
                    outputs = species_model(image_tensor)
                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                    confidence, predicted_class = torch.max(probabilities, 0)
                
                predicted_class = predicted_class.item()
                species_confidence = confidence.item()
                
                species_name = species_class_names[predicted_class] if predicted_class < len(species_class_names) else f"Class {predicted_class}"
                
                print(f"–†–∞—Å—Ç–µ–Ω–∏–µ {i+1} ({plant_type}): {species_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {species_confidence:.2%})")
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥—ã —Ä–∞—Å—Ç–µ–Ω–∏—è {i+1}: {e}")
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –î–ï–§–ï–ö–¢–û–í
        if defects_model is not None:
            try:
                defects_model.eval()
                with torch.no_grad():
                    outputs = defects_model(image_tensor)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ç–µ–Ω–∑–æ—Ä
                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                    confidence, predicted_class = torch.max(probabilities, 0)
                
                predicted_class = predicted_class.item()
                defects_confidence = confidence.item()
                
                defects_name = defects_class_names[predicted_class] if predicted_class < len(defects_class_names) else f"Class {predicted_class}"
                
                print(f"üîß –†–∞—Å—Ç–µ–Ω–∏–µ {i+1} - –î–µ—Ñ–µ–∫—Ç—ã: {defects_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {defects_confidence:.2%})")
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Ä–∞—Å—Ç–µ–Ω–∏—è {i+1}: {e}")
        
        classification_results.append({
            'box': (int(x1), int(y1), int(x2), int(y2)),
            'plant_type': plant_type,
            'detection_confidence': conf,
            'species': species_name,
            'species_confidence': species_confidence,
            'defects': defects_name,
            'defects_confidence': defects_confidence,
            'detection_class': detection_class,
            'plant_number': i+1  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Ä–∞—Å—Ç–µ–Ω–∏—è
        })
    
    return img, classification_results

def visualize_complete_detection(image, classification_results):
    img_display = image.copy()
    
    for result in classification_results:
        x1, y1, x2, y2 = result['box']
        species = result['species']
        defects = result['defects']
        det_conf = result['detection_confidence']
        species_conf = result['species_confidence']
        defects_conf = result['defects_confidence']
        plant_number = result['plant_number']
        
        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤
        if result['detection_class'] == 0:  # –î–µ—Ä–µ–≤–æ
            color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π
        else:  # –ö—É—Å—Ç
            color = (0, 165, 255)  # –û—Ä–∞–Ω–∂–µ–≤—ã–π
        
        # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
        cv2.rectangle(img_display, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
        
        # –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        number_text = f"{plant_number}"
        species_text = f"–ü–æ—Ä–æ–¥–∞: {species}"
        defects_text = f"–î–µ—Ñ–µ–∫—Ç—ã: {defects}"
        confidence_text = f"–î–µ—Ç–µ–∫—Ü–∏—è: {det_conf:.2f}, –ü–æ—Ä–æ–¥–∞: {species_conf:.2f}, –î–µ—Ñ–µ–∫—Ç—ã: {defects_conf:.2f}"
        
        # –†–∞–∑–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–∞
        number_size = cv2.getTextSize(number_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
        species_size = cv2.getTextSize(species_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        defects_size = cv2.getTextSize(defects_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        confidence_size = cv2.getTextSize(confidence_text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é –≤—ã—Å–æ—Ç—É –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞
        total_height = number_size[1] + species_size[1] + defects_size[1] + confidence_size[1] + 20
        max_width = max(number_size[0], species_size[0], defects_size[0], confidence_size[0])
        
        # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ - –ë–û–õ–ï–ï –Ø–†–ö–ò–ô –ò –ß–ï–¢–ö–ò–ô
        cv2.rectangle(img_display, 
                     (int(x1), int(y1) - total_height),
                     (int(x1) + max_width + 10, int(y1)),
                     (50, 50, 50), -1)  # –¢–µ–º–Ω–æ-—Å–µ—Ä—ã–π —Ñ–æ–Ω
        
        # –†–∞–º–∫–∞ –≤–æ–∫—Ä—É–≥ —Ñ–æ–Ω–∞
        cv2.rectangle(img_display, 
                     (int(x1), int(y1) - total_height),
                     (int(x1) + max_width + 10, int(y1)),
                     color, 2)
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏ —Ü–≤–µ—Ç–∞–º–∏
        y_offset = y1 - 5
        
        # –ù–æ–º–µ—Ä —Ä–∞—Å—Ç–µ–Ω–∏—è - –ö–†–£–ü–ù–´–ô –ò –Ø–†–ö–ò–ô
        cv2.putText(img_display, number_text, 
                   (int(x1) + 5, int(y_offset)), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        y_offset -= (number_size[1] + 5)
        
        # –ü–æ—Ä–æ–¥–∞
        cv2.putText(img_display, species_text,
                   (int(x1) + 5, int(y_offset)),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        y_offset -= (species_size[1] + 5)
        
        # –î–µ—Ñ–µ–∫—Ç—ã
        cv2.putText(img_display, defects_text,
                   (int(x1) + 5, int(y_offset)),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 100), 1)  # –ñ–µ–ª—Ç—ã–π —Ü–≤–µ—Ç –¥–ª—è –¥–µ—Ñ–µ–∫—Ç–æ–≤
        y_offset -= (defects_size[1] + 5)
        
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–º–µ–ª–∫–∏–º —à—Ä–∏—Ñ—Ç–æ–º)
        cv2.putText(img_display, confidence_text,
                   (int(x1) + 5, int(y_offset)),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
    
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    height, width = img_display.shape[:2]
    max_display_size = 1200
    if max(height, width) > max_display_size:
        scale = max_display_size / max(height, width)
        new_width = int(width * scale)
        new_height = int(height * scale)
        img_display = cv2.resize(img_display, (new_width, new_height))
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    window_name = "–î–µ—Ç–µ–∫—Ü–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞—Å—Ç–µ–Ω–∏–π (–Ω–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É)"
    cv2.imshow(window_name, img_display)
    print("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞–Ω–æ. –ù–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –≤ –æ–∫–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    
    import time
    time.sleep(0.5)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    output_path = "complete_detection_result.jpg"
    cv2.imwrite(output_path, cv2.cvtColor(img_display, cv2.COLOR_RGB2BGR))
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {output_path}")
    
    return img_display

def load_defects_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    defects_model = None
    defects_class_names = []
    
    try:
        checkpoint = torch.load('model_defects.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        defects_model = create_improved_model(num_classes)
        defects_model.load_state_dict(checkpoint['model_state_dict'])
        defects_class_names = checkpoint['class_names']
        print("–ú–æ–¥–µ–ª—å –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"–ú–æ–¥–µ–ª—å –¥–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    return defects_model, defects_class_names

def train_defects_model_improved(characteristiki_folder_path):

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤
    image_paths, labels, defect_descriptions = load_defects_data(characteristiki_folder_path)
    
    if len(image_paths) == 0:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤!")
        return None, None, [], [], []
    
    print(f"–î–∞–Ω–Ω—ã–µ –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(labels))} –∫–ª–∞—Å—Å–æ–≤")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–æ—Ä–æ–¥–∞–º)
    model, history, class_names = prepare_and_train_model(
        image_paths, labels, defect_descriptions, "–¥–µ—Ñ–µ–∫—Ç—ã"
    )
    
    return model, history, class_names, image_paths, labels

def load_defects_data(characteristiki_folder_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫/–¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    char_path = Path(characteristiki_folder_path)
    
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∏–∑: {char_path.absolute()}")
    
    if not char_path.exists():
        print(f"–ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {char_path}")
        return [], [], []
    
    # –ò—â–µ–º CSV —Ñ–∞–π–ª
    csv_path = char_path / "labels" / "labels.csv"
    if not csv_path.exists():
        csv_path = char_path / "labels.csv"
        if not csv_path.exists():
            print("CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return [], [], []
    
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
        print(f"CSV –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return [], [], []
    
    # –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    images_dir = char_path / "images"
    if not images_dir.exists():
        print("–ü–∞–ø–∫–∞ 'images' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return [], [], []
    
    images = []
    labels = []
    defect_descriptions = []
    
    successful = 0
    for _, row in df.iterrows():
        try:
            filename = str(row['filename']).strip()
            img_path = images_dir / filename
            
            if img_path.exists():
                images.append(str(img_path))
                labels.append(int(row['defect_label']))
                defect_descriptions.append(str(row['defect_description']))
                successful += 1
            else:
                print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}")
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏: {e}")
            continue
    
    
    return images, labels, defect_descriptions

def main_improved():
    porody_path = "data/–ø–æ—Ä–æ–¥—ã"
    char_path = "data/—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"  # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–µ—Ñ–µ–∫—Ç–æ–≤
    
    try:
        detection_model = YOLO("best.pt")
    except Exception as e:
        print(f"{e}")
        return
    


    tree_model, bush_model, tree_class_names, bush_class_names = load_separated_models()
    
    if tree_model is None and bush_model is None:

        tree_model, bush_model, tree_class_names, bush_class_names = train_separated_models(porody_path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ—Ä–æ–¥
        if tree_model is not None:
            torch.save({
                'model_state_dict': tree_model.state_dict(),
                'class_names': tree_class_names,
                'plant_type': 'tree'
            }, 'model_trees.pth')
            print("–ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
        if bush_model is not None:
            torch.save({
                'model_state_dict': bush_model.state_dict(),
                'class_names': bush_class_names, 
                'plant_type': 'bush'
            }, 'model_bushes.pth')
            print("–ú–æ–¥–µ–ª—å –∫—É—Å—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    # –ó–ê–ì–†–£–ó–ö–ê –ò–õ–ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–ï–§–ï–ö–¢–û–í
    print("\n–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –î–ï–§–ï–ö–¢–û–í...")
    defects_model, defects_class_names = load_defects_model()
    
    if defects_model is None:
        print("–û–ë–£–ß–ï–ù–ò–ï –ù–û–í–û–ô –ú–û–î–ï–õ–ò –î–ï–§–ï–ö–¢–û–í...")
        defects_model, defects_history, defects_class_names, _, _ = train_defects_model_improved(char_path)
        
        if defects_model is not None:
            torch.save({
                'model_state_dict': defects_model.state_dict(),
                'class_names': defects_class_names,
                'model_type': 'defects'
            }, 'model_defects.pth')
            print("–ú–æ–¥–µ–ª—å –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

if __name__ == "__main__":
    main_improved()