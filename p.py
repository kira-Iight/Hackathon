import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetB0
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import warnings
import os

# –û—Ç–∫–ª—é—á–∞–µ–º warnings –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º TensorFlow –¥–ª—è macOS
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# –û—Ç–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
IMG_SIZE = (224, 224)
BATCH_SIZE = 2
NUM_EPOCHS = 5  # –ï—â–µ –º–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

def load_tree_species_data(porody_folder_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥ –¥–µ—Ä–µ–≤—å–µ–≤"""
    porody_path = Path(porody_folder_path)
    
    print(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {porody_path.absolute()}")
    
    if not porody_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {porody_path}")
        return [], [], []
    
    # –ò—â–µ–º CSV —Ñ–∞–π–ª
    csv_path = porody_path / "labels" / "labels.csv"
    if not csv_path.exists():
        csv_path = porody_path / "labels.csv"
        if not csv_path.exists():
            print("‚ùå CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return [], [], []
    
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
        print(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return [], [], []
    
    # –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    images_dir = porody_path / "images"
    if not images_dir.exists():
        print("‚ùå –ü–∞–ø–∫–∞ 'images' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return [], [], []
    
    images = []
    labels = []
    species_names = []
    
    successful = 0
    for _, row in df.iterrows():
        try:
            filename = str(row['filename']).strip()
            img_path = images_dir / filename
            
            if img_path.exists():
                images.append(str(img_path))
                labels.append(int(row['species_label']))
                species_names.append(str(row['species_name']))
                successful += 1
            else:
                print(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏: {e}")
            continue
    
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {successful}/{len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(set(labels))}")
    
    return images, labels, species_names

def load_defects_data(characteristiki_folder_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫/–¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    char_path = Path(characteristiki_folder_path)
    
    print(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {char_path.absolute()}")
    
    if not char_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {char_path}")
        return [], [], []
    
    # –ò—â–µ–º CSV —Ñ–∞–π–ª
    csv_path = char_path / "labels" / "labels.csv"
    if not csv_path.exists():
        csv_path = char_path / "labels.csv"
        if not csv_path.exists():
            print("‚ùå CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return [], [], []
    
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except:
            df = pd.read_csv(csv_path, encoding='utf-8', on_bad_lines='skip')
        print(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return [], [], []
    
    # –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    images_dir = char_path / "images"
    if not images_dir.exists():
        print("‚ùå –ü–∞–ø–∫–∞ 'images' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return [], [], []
    
    images = []
    labels = []
    defect_descriptions = []
    
    successful = 0
    for _, row in df.iterrows():
        try:
            filename = str(row['filename']).strip()
            img_path = images_dir / filename
            
            if img_path.exists():
                images.append(str(img_path))
                labels.append(int(row['defect_label']))
                defect_descriptions.append(str(row['defect_description']))
                successful += 1
            else:
                print(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏: {e}")
            continue
    
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {successful}/{len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(set(labels))}")
    
    return images, labels, defect_descriptions

def load_and_preprocess_image(img_path, img_size=IMG_SIZE):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        img = keras.preprocessing.image.load_img(img_path, target_size=img_size)
        img_array = keras.preprocessing.image.img_to_array(img)
        img_array = img_array / 255.0
        return img_array
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}: {e}")
        return None

def create_simple_dataset(image_paths, labels, batch_size=2, img_size=IMG_SIZE):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏"""
    images = []
    valid_labels = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞—Ä–∞–Ω–µ–µ
    for i, img_path in enumerate(image_paths):
        img_array = load_and_preprocess_image(img_path, img_size)
        if img_array is not None:
            images.append(img_array)
            valid_labels.append(labels[i])
    
    images = np.array(images)
    valid_labels = np.array(valid_labels)
    
    # –°–æ–∑–¥–∞–µ–º tf.data.Dataset –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
    dataset = tf.data.Dataset.from_tensor_slices((images, valid_labels))
    dataset = dataset.batch(batch_size)
    
    return dataset

def create_simple_model(num_classes):
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤
    base_model = EfficientNetB0(
        weights=None,
        include_top=False,
        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)
    )
    datagen = keras.preprocessing.image.ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2)
    
    model = keras.Sequential([
    layers.Conv2D(32, 3, activation='relu', input_shape=(224,224,3)),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')])
    
    return model

def train_tree_species_model_simple(porody_folder_path):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥ –¥–µ—Ä–µ–≤—å–µ–≤"""
    
    print("üå≥ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ü–û–†–û–î –î–ï–†–ï–í–¨–ï–í")
    print("=" * 50)
    
    image_paths, labels, species_names = load_tree_species_data(porody_folder_path)
    
    if len(image_paths) == 0:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None, None, [], [], []
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(labels))} –∫–ª–∞—Å—Å–æ–≤")
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –º–µ—Ç–∫–∏ –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0..C-1
    unique_labels_sorted = sorted(set(labels))
    original_to_compact = {orig: idx for idx, orig in enumerate(unique_labels_sorted)}
    labels_mapped_all = [original_to_compact[l] for l in labels]

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤
    class_names = []
    for orig_label in unique_labels_sorted:
        for path_i, lab in enumerate(labels):
            if lab == orig_label:
                class_names.append(species_names[path_i])
                break

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if len(image_paths) <= 3:
        print("‚ö†Ô∏è –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_paths, train_labels = image_paths, labels_mapped_all
        val_paths, val_labels = image_paths, labels_mapped_all
    else:
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            image_paths, labels_mapped_all, test_size=0.2, random_state=42
        )
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_paths)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {len(val_paths)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    batch_size = min(BATCH_SIZE, len(train_paths))
    if batch_size == 0:
        batch_size = 1
    
    train_dataset = create_simple_dataset(train_paths, train_labels, batch_size)
    val_dataset = create_simple_dataset(val_paths, val_labels, batch_size)
    
    num_classes = len(unique_labels_sorted)
    model = create_simple_model(num_classes)

    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–†–û–î...")

    # –û–±—É—á–µ–Ω–∏–µ
    history = model.fit(
        train_dataset,
        epochs=min(NUM_EPOCHS, 5),
        validation_data=val_dataset,
        verbose=1
    )

    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    return model, history, class_names, image_paths, labels_mapped_all

def train_defects_model_simple(characteristiki_folder_path):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫/–¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    
    print("üîç –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö")
    print("=" * 50)
    
    image_paths, labels, defect_descriptions = load_defects_data(characteristiki_folder_path)
    
    if len(image_paths) == 0:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None, None, [], [], []
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(labels))} –∫–ª–∞—Å—Å–æ–≤")
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –º–µ—Ç–∫–∏ –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0..C-1
    unique_labels_sorted = sorted(set(labels))
    original_to_compact = {orig: idx for idx, orig in enumerate(unique_labels_sorted)}
    labels_mapped_all = [original_to_compact[l] for l in labels]

    # –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤
    class_names = []
    for orig_label in unique_labels_sorted:
        for path_i, lab in enumerate(labels):
            if lab == orig_label:
                class_names.append(defect_descriptions[path_i])
                break

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if len(image_paths) <= 3:
        print("‚ö†Ô∏è –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_paths, train_labels = image_paths, labels_mapped_all
        val_paths, val_labels = image_paths, labels_mapped_all
    else:
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            image_paths, labels_mapped_all, test_size=0.2, random_state=42
        )
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_paths)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {len(val_paths)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    batch_size = min(BATCH_SIZE, len(train_paths))
    if batch_size == 0:
        batch_size = 1
    
    train_dataset = create_simple_dataset(train_paths, train_labels, batch_size)
    val_dataset = create_simple_dataset(val_paths, val_labels, batch_size)
    
    num_classes = len(unique_labels_sorted)
    model = create_simple_model(num_classes)

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö...")

    history = model.fit(
        train_dataset,
        epochs=min(NUM_EPOCHS, 5),
        validation_data=val_dataset,
        verbose=1
    )

    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    return model, history, class_names, image_paths, labels_mapped_all

def simple_test_model(model, test_image_path, class_names, model_type="–ø–æ—Ä–æ–¥—ã"):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        img_array = load_and_preprocess_image(test_image_path)
        if img_array is None:
            return None, 0
            
        img_array = np.expand_dims(img_array, axis=0)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = model.predict(img_array, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = np.max(predictions[0])
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞
        if predicted_class < len(class_names):
            class_name = class_names[predicted_class]
        else:
            class_name = f"–ö–ª–∞—Å—Å {predicted_class}"
        
        print(f"\nüîç –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ({model_type}):")
        print(f"üì∏ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {Path(test_image_path).name}")
        print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {class_name}")
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
        
        return class_name, confidence
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return None, 0

def evaluate_model_on_all_images(model, image_paths, true_labels, class_names, model_type="–ø–æ—Ä–æ–¥—ã"):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –≤—ã–≤–æ–¥–æ–º ‚úÖ/‚ùå –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    print(f"\nüìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ù–ê –í–°–ï–• –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–• ({model_type}):")
    print("=" * 60)
    
    if len(image_paths) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
        return 0
    
    correct_predictions = 0
    total_images = len(image_paths)
    
    print(f"üî¢ –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {total_images}")
    print("-" * 60)
    
    for i, (img_path, true_label) in enumerate(zip(image_paths, true_labels)):
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img_array = load_and_preprocess_image(img_path)
            if img_array is None:
                continue
                
            img_array = np.expand_dims(img_array, axis=0)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            predictions = model.predict(img_array, verbose=0)
            predicted_class = np.argmax(predictions[0])
            confidence = np.max(predictions[0])
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            is_correct = (predicted_class == true_label)
            if is_correct:
                correct_predictions += 1
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
            true_class_name = class_names[true_label] if true_label < len(class_names) else f"Class {true_label}"
            pred_class_name = class_names[predicted_class] if predicted_class < len(class_names) else f"Class {predicted_class}"
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å —ç–º–æ–¥–∑–∏
            status = "‚úÖ" if is_correct else "‚ùå"
            print(f"{status} {i+1:2d}/{total_images}: {Path(img_path).name:15} | "
                  f"–ò—Å—Ç–∏–Ω–∞: {true_class_name:20} | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {pred_class_name:20} | "
                  f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ {img_path}: {e}")
            continue
    
    accuracy = correct_predictions / total_images if total_images > 0 else 0
    print("-" * 60)
    print(f"üéØ –ò–¢–û–ì–û–í–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.2%} ({correct_predictions}/{total_images})")
    
    return accuracy

# –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
if __name__ == "__main__":
    print("üå≤ –ó–ê–ü–£–°–ö –£–ü–†–û–©–ï–ù–ù–û–ô –°–ò–°–¢–ï–ú–´ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –î–ï–†–ï–í–¨–ï–í")
    print("=" * 60)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    tf.config.set_soft_device_placement(True)
    
    porody_path = "data/–ø–æ—Ä–æ–¥—ã"
    char_path = "data/—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
    
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫
        if not Path(porody_path).exists():
            print(f"‚ùå –ü–∞–ø–∫–∞ —Å –ø–æ—Ä–æ–¥–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {porody_path}")
            porody_path = input("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –ø–æ—Ä–æ–¥–∞–º–∏: ")
        
        if not Path(char_path).exists():
            print(f"‚ùå –ü–∞–ø–∫–∞ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {char_path}")
            char_path = input("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏: ")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Ä–æ–¥
        porody_model, porody_history, species_names, porody_images, porody_labels = train_tree_species_model_simple(porody_path)
        
        print("\n" + "=" * 60)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã)
        defects_model, defects_history, defect_descriptions, defects_images, defects_labels = train_defects_model_simple(char_path)
        
        print("\n" + "=" * 60)
        print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("=" * 60)
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Ä–æ–¥
        if porody_model is not None and len(species_names) > 0 and len(porody_images) > 0:
            print("üå≥ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–†–û–î:")
            test_image = porody_images[0]
            simple_test_model(porody_model, test_image, species_names, "–ø–æ—Ä–æ–¥—ã")
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
            porody_accuracy = evaluate_model_on_all_images(
                porody_model, porody_images, porody_labels, species_names, "–ø–æ—Ä–æ–¥—ã"
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            try:
                porody_model.save('model_porody_simple.h5')
                print("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ—Ä–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_porody_simple.h5'")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Ä–æ–¥: {e}")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if defects_model is not None and len(defect_descriptions) > 0 and len(defects_images) > 0:
            print("\nüîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö:")
            test_image = defects_images[0]
            simple_test_model(defects_model, test_image, defect_descriptions, "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
            defects_accuracy = evaluate_model_on_all_images(
                defects_model, defects_images, defects_labels, defect_descriptions, "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            try:
                defects_model.save('model_defects_simple.h5')
                print("‚úÖ –ú–æ–¥–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_defects_simple.h5'")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {e}")
        else:
            print("\n‚ö†Ô∏è –ú–æ–¥–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –Ω–µ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å –¥–∞–Ω–Ω—ã–º–∏")
        
        print("\nüéâ –ü–†–û–ì–†–ê–ú–ú–ê –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!")
        
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É")





