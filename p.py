import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetB0
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import warnings
<<<<<<< HEAD
import os

# –û—Ç–∫–ª—é—á–∞–µ–º warnings –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º TensorFlow –¥–ª—è macOS
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# –û—Ç–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)
=======
warnings.filterwarnings('ignore')
>>>>>>> e4a4378eac530946868097685580eb82d315742b

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
IMG_SIZE = (224, 224)
BATCH_SIZE = 2
<<<<<<< HEAD
NUM_EPOCHS = 5  # –ï—â–µ –º–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
=======
NUM_EPOCHS = 50
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º (fine-tuning)
WARMUP_EPOCHS = 8
FINE_TUNE_LR = 5e-5

def create_proper_csv_files():
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ CSV —Ñ–∞–π–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ—é—â–∏—Ö—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    # –ü–∞–ø–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
    porody_path = Path("data/–ø–æ—Ä–æ–¥—ã")
    char_path = Path("data/—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
    
    # –°–æ–∑–¥–∞–µ–º CSV –¥–ª—è –ø–æ—Ä–æ–¥
    porody_images_dir = porody_path / "images"
    if porody_images_dir.exists():
        images = list(porody_images_dir.glob("*.jpg")) + list(porody_images_dir.glob("*.png")) + list(porody_images_dir.glob("*.jpeg"))
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Ä–æ–¥")
        
        # –°–æ–∑–¥–∞–µ–º mapping –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
        porody_data = []
        for i, img_path in enumerate(images):
            img_name = img_path.stem
            
            # –ü—Ä–æ—Å—Ç–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –Ω–æ–º–µ—Ä—É —Ñ–∞–π–ª–∞
            species_name = f"–ü–æ—Ä–æ–¥–∞_{img_name}"
            if img_name == "1":
                species_name = "–ö–ª–µ–Ω –æ—Å—Ç—Ä–æ–ª–∏—Å—Ç–Ω—ã–π"
            elif img_name == "2":
                species_name = "–õ–∏—Å—Ç–≤–µ–Ω–Ω–∏—Ü–∞"
            elif img_name == "3":
                species_name = "–¢—É—è"
            elif img_name == "4":
                species_name = "–†—è–±–∏–Ω–∞"
            elif img_name == "5":
                species_name = "–°–æ—Å–Ω–∞"
            elif img_name == "6":
                species_name = "–ú–æ–∂–∂–µ–≤–µ–ª—å–Ω–∏–∫"
            elif img_name == "7":
                species_name = "–ë–µ—Ä–µ–∑–∞"
            elif img_name == "8":
                species_name = "–ö–∞—à—Ç–∞–Ω"
            elif img_name == "9":
                species_name = "–ò–≤–∞"
            elif img_name == "10":
                species_name = "–û—Å–∏–Ω–∞"
            
            porody_data.append({
                'filename': img_path.name,
                'species_label': i,  # –£–Ω–∏–∫–∞–ª—å–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                'species_name': species_name
            })
        
        porody_df = pd.DataFrame(porody_data)
        porody_df.to_csv(porody_path / "proper_labels.csv", index=False, encoding='utf-8')
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω CSV –¥–ª—è –ø–æ—Ä–æ–¥: {len(porody_data)} –∑–∞–ø–∏—Å–µ–π")
    
    # –°–æ–∑–¥–∞–µ–º CSV –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    char_images_dir = char_path / "images"
    if char_images_dir.exists():
        images = list(char_images_dir.glob("*.jpg")) + list(char_images_dir.glob("*.png")) + list(char_images_dir.glob("*.jpeg"))
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
        
        char_data = []
        for i, img_path in enumerate(images):
            img_name = img_path.stem
            
            # –ü—Ä–æ—Å—Ç–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            defect_description = f"–î–µ—Ñ–µ–∫—Ç_{img_name}"
            if img_name == "1":
                defect_description = "–ö–æ–º–ª–µ–≤–∞—è –≥–Ω–∏–ª—å"
            elif img_name == "2":
                defect_description = "–°—É—Ö–æ–±–æ—á–∏–Ω–∞"
            elif img_name == "3":
                defect_description = "–°—Ç–≤–æ–ª–æ–≤–∞—è –≥–Ω–∏–ª—å"
            elif img_name == "4":
                defect_description = "–ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è"
            elif img_name == "5":
                defect_description = "–ü–ª–æ–¥–æ–≤—ã–µ —Ç–µ–ª–∞"
            elif img_name == "6":
                defect_description = "–û—Ç—Å–ª–æ–µ–Ω–∏–µ –∫–æ—Ä—ã"
            elif img_name == "7":
                defect_description = "–°—É—Ö–∏–µ –≤–µ—Ç–≤–∏"
            elif img_name == "8":
                defect_description = "–°—É—Ö–æ—Å—Ç–æ–π"
            elif img_name == "9":
                defect_description = "–î—É–ø–ª–æ"
            elif img_name == "10":
                defect_description = "–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –≤—Ä–µ–¥–∏—Ç–µ–ª—è–º–∏"
            
            char_data.append({
                'filename': img_path.name,
                'defect_label': i,  # –£–Ω–∏–∫–∞–ª—å–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                'defect_description': defect_description
            })
        
        char_df = pd.DataFrame(char_data)
        char_df.to_csv(char_path / "proper_labels.csv", index=False, encoding='utf-8')
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω CSV –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(char_data)} –∑–∞–ø–∏—Å–µ–π")
>>>>>>> e4a4378eac530946868097685580eb82d315742b

def load_tree_species_data(porody_folder_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥ –¥–µ—Ä–µ–≤—å–µ–≤"""
    porody_path = Path(porody_folder_path)
    
    print(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {porody_path.absolute()}")
    
    if not porody_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {porody_path}")
        return [], [], []
    
<<<<<<< HEAD
    # –ò—â–µ–º CSV —Ñ–∞–π–ª
    csv_path = porody_path / "labels" / "labels.csv"
    if not csv_path.exists():
        csv_path = porody_path / "labels.csv"
        if not csv_path.exists():
            print("‚ùå CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
=======
    # –ò—â–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π CSV —Ñ–∞–π–ª
    csv_path = porody_path / "proper_labels.csv"
    if not csv_path.exists():
        print("‚ùå proper_labels.csv –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º...")
        create_proper_csv_files()
        
        if not csv_path.exists():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å CSV —Ñ–∞–π–ª")
>>>>>>> e4a4378eac530946868097685580eb82d315742b
            return [], [], []
    
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
        print(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
<<<<<<< HEAD
=======
        print("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ CSV:")
        print(df.head())
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return [], [], []
    
    # –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    images_dir = porody_path / "images"
    if not images_dir.exists():
        print("‚ùå –ü–∞–ø–∫–∞ 'images' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return [], [], []
    
    images = []
    labels = []
    species_names = []
    
    successful = 0
    for _, row in df.iterrows():
        try:
            filename = str(row['filename']).strip()
            img_path = images_dir / filename
            
            if img_path.exists():
                images.append(str(img_path))
                labels.append(int(row['species_label']))
                species_names.append(str(row['species_name']))
                successful += 1
            else:
                print(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏: {e}")
            continue
    
<<<<<<< HEAD
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {successful}/{len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
=======
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {successful}/{len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Ä–æ–¥")
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(set(labels))}")
    
    return images, labels, species_names

def load_defects_data(characteristiki_folder_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫/–¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    char_path = Path(characteristiki_folder_path)
    
    print(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {char_path.absolute()}")
    
    if not char_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {char_path}")
        return [], [], []
    
<<<<<<< HEAD
    # –ò—â–µ–º CSV —Ñ–∞–π–ª
    csv_path = char_path / "labels" / "labels.csv"
    if not csv_path.exists():
        csv_path = char_path / "labels.csv"
        if not csv_path.exists():
            print("‚ùå CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return [], [], []
    
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except:
            df = pd.read_csv(csv_path, encoding='utf-8', on_bad_lines='skip')
        print(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
=======
    # –ò—â–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π CSV —Ñ–∞–π–ª
    csv_path = char_path / "proper_labels.csv"
    if not csv_path.exists():
        print("‚ùå proper_labels.csv –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º...")
        create_proper_csv_files()
        
        if not csv_path.exists():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å CSV —Ñ–∞–π–ª")
            return [], [], []
    
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
        print(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        print("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ CSV:")
        print(df.head())
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return [], [], []
    
    # –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    images_dir = char_path / "images"
    if not images_dir.exists():
        print("‚ùå –ü–∞–ø–∫–∞ 'images' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return [], [], []
    
    images = []
    labels = []
    defect_descriptions = []
    
    successful = 0
    for _, row in df.iterrows():
        try:
            filename = str(row['filename']).strip()
            img_path = images_dir / filename
            
            if img_path.exists():
                images.append(str(img_path))
                labels.append(int(row['defect_label']))
                defect_descriptions.append(str(row['defect_description']))
                successful += 1
            else:
                print(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏: {e}")
            continue
    
<<<<<<< HEAD
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {successful}/{len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
=======
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {successful}/{len(df)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(set(labels))}")
    
    return images, labels, defect_descriptions

<<<<<<< HEAD
def load_and_preprocess_image(img_path, img_size=IMG_SIZE):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        img = keras.preprocessing.image.load_img(img_path, target_size=img_size)
        img_array = keras.preprocessing.image.img_to_array(img)
        img_array = img_array / 255.0
        return img_array
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}: {e}")
        return None

def create_simple_dataset(image_paths, labels, batch_size=2, img_size=IMG_SIZE):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏"""
    images = []
    valid_labels = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞—Ä–∞–Ω–µ–µ
    for i, img_path in enumerate(image_paths):
        img_array = load_and_preprocess_image(img_path, img_size)
        if img_array is not None:
            images.append(img_array)
            valid_labels.append(labels[i])
    
    images = np.array(images)
    valid_labels = np.array(valid_labels)
    
    # –°–æ–∑–¥–∞–µ–º tf.data.Dataset –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
    dataset = tf.data.Dataset.from_tensor_slices((images, valid_labels))
    dataset = dataset.batch(batch_size)
    
    return dataset

def create_simple_model(num_classes):
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤
    base_model = EfficientNetB0(
        weights=None,
        include_top=False,
        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)
    )
    datagen = keras.preprocessing.image.ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2)
    
    model = keras.Sequential([
    layers.Conv2D(32, 3, activation='relu', input_shape=(224,224,3)),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')])
    
    return model

def train_tree_species_model_simple(porody_folder_path):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥ –¥–µ—Ä–µ–≤—å–µ–≤"""
=======
class AdvancedDataGenerator(keras.utils.Sequence):
    def __init__(self, image_paths, labels, batch_size=2, img_size=(224, 224), 
                 shuffle=True, augmentation=True):
        self.image_paths = image_paths
        self.labels = labels
        self.batch_size = batch_size
        self.img_size = img_size
        self.shuffle = shuffle
        self.augmentation = augmentation
        self.on_epoch_end()
        
        if self.augmentation:
            self.datagen = keras.preprocessing.image.ImageDataGenerator(
                rotation_range=20,
                width_shift_range=0.1,
                height_shift_range=0.1,
                zoom_range=0.1,
                horizontal_flip=True,
                fill_mode='nearest'
            )
        else:
            self.datagen = keras.preprocessing.image.ImageDataGenerator()
    
    def __len__(self):
        return int(np.ceil(len(self.image_paths) / self.batch_size))
    
    def __getitem__(self, index):
        batch_paths = self.image_paths[index*self.batch_size:(index+1)*self.batch_size]
        batch_labels = self.labels[index*self.batch_size:(index+1)*self.batch_size]
        
        X = []
        y = []
        
        for i, img_path in enumerate(batch_paths):
            try:
                img = keras.preprocessing.image.load_img(img_path, target_size=self.img_size)
                img_array = keras.preprocessing.image.img_to_array(img)
                img_array = img_array / 255.0
                
                X.append(img_array)
                y.append(batch_labels[i])
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}: {e}")
                continue
        
        if not X:
            return np.zeros((1, self.img_size[0], self.img_size[1], 3)), np.zeros(1)
        
        X = np.array(X)
        y = np.array(y)
        
        if self.augmentation:
            X = self.datagen.flow(X, batch_size=len(X), shuffle=False).next()
        
        return X, y
    
    def on_epoch_end(self):
        if self.shuffle:
            indices = np.arange(len(self.image_paths))
            np.random.shuffle(indices)
            self.image_paths = [self.image_paths[i] for i in indices]
            self.labels = [self.labels[i] for i in indices]

def create_regularized_model(num_classes, model_name='tree_species'):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π"""
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ ImageNet, –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
    try:
        base_model = EfficientNetB0(
            weights='imagenet',
            include_top=False,
            input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)
        )
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ ImageNet –¥–ª—è EfficientNetB0: {e}\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤.")
        base_model = EfficientNetB0(
            weights=None,
            include_top=False,
            input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)
        )
    base_model.trainable = False
    
    model = keras.Sequential([
        keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),
        layers.Rescaling(1./255),
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dropout(0.5),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

def _get_base_model_from_sequential(model: keras.Model):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å EfficientNet –≤–Ω—É—Ç—Ä–∏ Sequential."""
    for layer in model.layers:
        if isinstance(layer, keras.Model) and layer.name.startswith('efficientnet'):
            return layer
    return None

def _compute_class_weights(labels):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    if not labels:
        return None
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –º–µ—Ç–∫–∏ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 0 –∏ –∏–¥—É—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    unique_labels = sorted(set(labels))
    num_classes = len(unique_labels)
    
    # –ï—Å–ª–∏ –º–µ—Ç–∫–∏ —É–∂–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0..num_classes-1
    if min(labels) == 0 and max(labels) == num_classes - 1:
        unique, counts = np.unique(labels, return_counts=True)
    else:
        # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        label_mapping = {old: new for new, old in enumerate(unique_labels)}
        remapped_labels = [label_mapping[label] for label in labels]
        unique, counts = np.unique(remapped_labels, return_counts=True)
    
    total = np.sum(counts)
    class_weights = {}
    
    for cls, cnt in zip(unique, counts):
        class_weights[int(cls)] = float(total / (num_classes * cnt))
    
    print(f"üìä –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weights}")
    return class_weights

def _enable_fine_tuning(model: keras.Model, trainable_ratio: float = 0.2):
    """–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤–µ—Ä—Ö–Ω—é—é —á–∞—Å—Ç—å EfficientNet –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è."""
    base_model = _get_base_model_from_sequential(model)
    if base_model is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å EfficientNet")
        return False
    
    # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω–∏–µ —Å–ª–æ–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ—Ç–∞
    total_layers = len(base_model.layers)
    trainable_from = int(total_layers * (1.0 - trainable_ratio))
    
    print(f"üõ†Ô∏è –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º {total_layers - trainable_from} –∏–∑ {total_layers} —Å–ª–æ–µ–≤")
    
    for i, layer in enumerate(base_model.layers):
        layer.trainable = (i >= trainable_from)
        if layer.trainable:
            print(f"   ‚úÖ –°–ª–æ–π {i}: {layer.name} - —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω")
    
    return True

def train_tree_species_model(porody_folder_path):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥ –¥–µ—Ä–µ–≤—å–µ–≤"""
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    
    print("üå≥ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ü–û–†–û–î –î–ï–†–ï–í–¨–ï–í")
    print("=" * 50)
    
    image_paths, labels, species_names = load_tree_species_data(porody_folder_path)
    
    if len(image_paths) == 0:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None, None, [], [], []
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(labels))} –∫–ª–∞—Å—Å–æ–≤")
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –º–µ—Ç–∫–∏ –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0..C-1
    unique_labels_sorted = sorted(set(labels))
    original_to_compact = {orig: idx for idx, orig in enumerate(unique_labels_sorted)}
    labels_mapped_all = [original_to_compact[l] for l in labels]

<<<<<<< HEAD
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤
    class_names = []
    for orig_label in unique_labels_sorted:
=======
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ compact-–º–µ—Ç–æ–∫
    class_names = []
    for orig_label in unique_labels_sorted:
        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ª–µ–π–±–ª–∞
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        for path_i, lab in enumerate(labels):
            if lab == orig_label:
                class_names.append(species_names[path_i])
                break

<<<<<<< HEAD
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if len(image_paths) <= 3:
        print("‚ö†Ô∏è –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_paths, train_labels = image_paths, labels_mapped_all
        val_paths, val_labels = image_paths, labels_mapped_all
    else:
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            image_paths, labels_mapped_all, test_size=0.2, random_state=42
        )
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_paths)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {len(val_paths)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
=======
    # –î–ª—è –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    if len(image_paths) <= 5:
        print("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_paths, train_labels = image_paths, labels_mapped_all
        val_paths, val_labels = image_paths[:1], labels_mapped_all[:1]  # –û–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    else:
        # –î–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        try:
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                image_paths, labels_mapped_all, test_size=0.2, random_state=42, stratify=labels_mapped_all
            )
        except:
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å stratify, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ–≥–æ
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                image_paths, labels_mapped_all, test_size=0.2, random_state=42, stratify=None
            )
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_paths)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {len(val_paths)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ batch_size –Ω–µ –±–æ–ª—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    batch_size = min(BATCH_SIZE, len(train_paths))
    if batch_size == 0:
        batch_size = 1
    
<<<<<<< HEAD
    train_dataset = create_simple_dataset(train_paths, train_labels, batch_size)
    val_dataset = create_simple_dataset(val_paths, val_labels, batch_size)
    
    num_classes = len(unique_labels_sorted)
    model = create_simple_model(num_classes)

    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
=======
    train_gen = AdvancedDataGenerator(train_paths, train_labels, 
                                    batch_size=batch_size, 
                                    img_size=IMG_SIZE, augmentation=True)
    
    val_gen = AdvancedDataGenerator(val_paths, val_labels, 
                                  batch_size=min(BATCH_SIZE, len(val_paths)), 
                                  img_size=IMG_SIZE, augmentation=False, shuffle=False)
    
    num_classes = len(unique_labels_sorted)
    model = create_regularized_model(num_classes, 'tree_species')

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
    class_weights = _compute_class_weights(train_labels)
    if class_weights:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–ª—é—á–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥–∏–∞–ø–∞–∑–æ–Ω—É –∫–ª–∞—Å—Å–æ–≤
        expected_keys = set(range(num_classes))
        actual_keys = set(class_weights.keys())
        if expected_keys != actual_keys:
            print(f"‚ö†Ô∏è –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: –æ–∂–∏–¥–∞–ª–∏—Å—å {expected_keys}, –ø–æ–ª—É—á–µ–Ω—ã {actual_keys}")
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤–µ—Å–∞
            correct_weights = {}
            for i in range(num_classes):
                if i in class_weights:
                    correct_weights[i] = class_weights[i]
                else:
                    correct_weights[i] = 1.0  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            class_weights = correct_weights

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),  # –£–≤–µ–ª–∏—á–∏–º learning rate –¥–ª—è warmup
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

<<<<<<< HEAD
    print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–†–û–î...")

    # –û–±—É—á–µ–Ω–∏–µ
    history = model.fit(
        train_dataset,
        epochs=min(NUM_EPOCHS, 5),
        validation_data=val_dataset,
        verbose=1
    )

    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    return model, history, class_names, image_paths, labels_mapped_all

def train_defects_model_simple(characteristiki_folder_path):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫/–¥–µ—Ñ–µ–∫—Ç–æ–≤"""
=======
    checkpoint_cb = keras.callbacks.ModelCheckpoint(
        filepath='best_tree_species.keras',
        monitor='val_accuracy',
        mode='max',
        save_best_only=True,
        save_weights_only=False
    )

    callbacks = [
        keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(patience=8, factor=0.5, min_lr=1e-6),
        checkpoint_cb,
    ]

    print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–†–û–î (WARMUP)...")

    epochs = min(NUM_EPOCHS, 30)  # –£–º–µ–Ω—å—à–∏–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    warmup_epochs = min(WARMUP_EPOCHS, max(5, epochs // 3))  # –£–≤–µ–ª–∏—á–∏–º warmup

    # –ü–µ—Ä–≤—ã–π —ç—Ç–∞–ø: –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    history = model.fit(
        train_gen,
        epochs=warmup_epochs,
        validation_data=val_gen,
        callbacks=callbacks,
        class_weight=class_weights,
        verbose=1
    )

    # Fine-tuning
    print("üõ†Ô∏è –î–û–û–±—É—á–µ–Ω–∏–µ (fine-tuning) –≤–µ—Ä—Ö–Ω–∏—Ö —Å–ª–æ—ë–≤ EfficientNet...")
    fine_tune_enabled = _enable_fine_tuning(model, trainable_ratio=0.2)  # –£–º–µ–Ω—å—à–∏–º ratio
    
    if fine_tune_enabled:
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Å –º–µ–Ω—å—à–∏–º learning rate –¥–ª—è fine-tuning
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=FINE_TUNE_LR),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        remaining_epochs = max(5, epochs - warmup_epochs)  # –ú–∏–Ω–∏–º—É–º 5 —ç–ø–æ—Ö –¥–ª—è fine-tuning
        
        history_ft = model.fit(
            train_gen,
            epochs=remaining_epochs,
            validation_data=val_gen,
            callbacks=callbacks,
            class_weight=class_weights,
            verbose=1
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        for k in history_ft.history.keys():
            if k in history.history:
                history.history[k].extend(history_ft.history[k])

    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    return model, history, class_names, image_paths, labels_mapped_all

def train_defects_model(characteristiki_folder_path):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫/–¥–µ—Ñ–µ–∫—Ç–æ–≤"""
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    
    print("üîç –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö")
    print("=" * 50)
    
    image_paths, labels, defect_descriptions = load_defects_data(characteristiki_folder_path)
    
    if len(image_paths) == 0:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None, None, [], [], []
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(set(labels))} –∫–ª–∞—Å—Å–æ–≤")
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –º–µ—Ç–∫–∏ –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0..C-1
    unique_labels_sorted = sorted(set(labels))
    original_to_compact = {orig: idx for idx, orig in enumerate(unique_labels_sorted)}
    labels_mapped_all = [original_to_compact[l] for l in labels]

<<<<<<< HEAD
    # –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤
=======
    # –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ compact-–º–µ—Ç–æ–∫
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    class_names = []
    for orig_label in unique_labels_sorted:
        for path_i, lab in enumerate(labels):
            if lab == orig_label:
                class_names.append(defect_descriptions[path_i])
                break

<<<<<<< HEAD
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if len(image_paths) <= 3:
        print("‚ö†Ô∏è –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_paths, train_labels = image_paths, labels_mapped_all
        val_paths, val_labels = image_paths, labels_mapped_all
    else:
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            image_paths, labels_mapped_all, test_size=0.2, random_state=42
        )
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_paths)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {len(val_paths)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
=======
    # –î–ª—è –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    if len(image_paths) <= 5:
        print("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_paths, train_labels = image_paths, labels_mapped_all
        val_paths, val_labels = image_paths[:1], labels_mapped_all[:1]
    else:
        try:
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                image_paths, labels_mapped_all, test_size=0.2, random_state=42, stratify=labels_mapped_all
            )
        except:
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                image_paths, labels_mapped_all, test_size=0.2, random_state=42, stratify=None
            )
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_paths)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {len(val_paths)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    batch_size = min(BATCH_SIZE, len(train_paths))
    if batch_size == 0:
        batch_size = 1
    
<<<<<<< HEAD
    train_dataset = create_simple_dataset(train_paths, train_labels, batch_size)
    val_dataset = create_simple_dataset(val_paths, val_labels, batch_size)
    
    num_classes = len(unique_labels_sorted)
    model = create_simple_model(num_classes)

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
=======
    train_gen = AdvancedDataGenerator(train_paths, train_labels, 
                                    batch_size=batch_size, 
                                    img_size=IMG_SIZE, augmentation=True)
    
    val_gen = AdvancedDataGenerator(val_paths, val_labels, 
                                  batch_size=min(BATCH_SIZE, len(val_paths)), 
                                  img_size=IMG_SIZE, augmentation=False, shuffle=False)
    
    num_classes = len(unique_labels_sorted)
    model = create_regularized_model(num_classes, 'defects')

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
    class_weights = _compute_class_weights(train_labels)
    if class_weights:
        expected_keys = set(range(num_classes))
        actual_keys = set(class_weights.keys())
        if expected_keys != actual_keys:
            print(f"‚ö†Ô∏è –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: –æ–∂–∏–¥–∞–ª–∏—Å—å {expected_keys}, –ø–æ–ª—É—á–µ–Ω—ã {actual_keys}")
            correct_weights = {}
            for i in range(num_classes):
                if i in class_weights:
                    correct_weights[i] = class_weights[i]
                else:
                    correct_weights[i] = 1.0
            class_weights = correct_weights

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

<<<<<<< HEAD
    print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö...")

    history = model.fit(
        train_dataset,
        epochs=min(NUM_EPOCHS, 5),
        validation_data=val_dataset,
        verbose=1
    )

    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    return model, history, class_names, image_paths, labels_mapped_all

def simple_test_model(model, test_image_path, class_names, model_type="–ø–æ—Ä–æ–¥—ã"):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        img_array = load_and_preprocess_image(test_image_path)
        if img_array is None:
            return None, 0
            
        img_array = np.expand_dims(img_array, axis=0)
=======
    checkpoint_cb = keras.callbacks.ModelCheckpoint(
        filepath='best_defects.keras',
        monitor='val_accuracy',
        mode='max',
        save_best_only=True,
        save_weights_only=False
    )

    callbacks = [
        keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(patience=8, factor=0.5, min_lr=1e-6),
        checkpoint_cb,
    ]

    print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö (WARMUP)...")

    epochs = min(NUM_EPOCHS, 30)
    warmup_epochs = min(WARMUP_EPOCHS, max(5, epochs // 3))

    history = model.fit(
        train_gen,
        epochs=warmup_epochs,
        validation_data=val_gen,
        callbacks=callbacks,
        class_weight=class_weights,
        verbose=1
    )

    # Fine-tuning
    print("üõ†Ô∏è –î–û–û–±—É—á–µ–Ω–∏–µ (fine-tuning) –≤–µ—Ä—Ö–Ω–∏—Ö —Å–ª–æ—ë–≤ EfficientNet...")
    fine_tune_enabled = _enable_fine_tuning(model, trainable_ratio=0.2)
    
    if fine_tune_enabled:
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=FINE_TUNE_LR),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        remaining_epochs = max(5, epochs - warmup_epochs)
        
        history_ft = model.fit(
            train_gen,
            epochs=remaining_epochs,
            validation_data=val_gen,
            callbacks=callbacks,
            class_weight=class_weights,
            verbose=1
        )
        
        for k in history_ft.history.keys():
            if k in history.history:
                history.history[k].extend(history_ft.history[k])

    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    return model, history, class_names, image_paths, labels_mapped_all

def plot_training_history(history, title):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    if 'val_accuracy' in history.history:
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{title} - Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    if 'val_loss' in history.history:
        plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{title} - Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

def test_model(model, test_image_path, class_names, model_type="–ø–æ—Ä–æ–¥—ã"):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        img = keras.preprocessing.image.load_img(test_image_path, target_size=IMG_SIZE)
        img_array = keras.preprocessing.image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0) / 255.0
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = model.predict(img_array, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = np.max(predictions[0])
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞
        if predicted_class < len(class_names):
            class_name = class_names[predicted_class]
        else:
            class_name = f"–ö–ª–∞—Å—Å {predicted_class}"
        
<<<<<<< HEAD
=======
        # –¢–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        top3_indices = np.argsort(predictions[0])[-3:][::-1]
        top3_predictions = []
        for idx in top3_indices:
            if idx < len(class_names):
                name = class_names[idx]
            else:
                name = f"–ö–ª–∞—Å—Å {idx}"
            top3_predictions.append((name, predictions[0][idx]))
        
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        print(f"\nüîç –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ({model_type}):")
        print(f"üì∏ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {Path(test_image_path).name}")
        print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {class_name}")
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
<<<<<<< HEAD
=======
        print(f"üèÜ –¢–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
        for i, (name, conf) in enumerate(top3_predictions, 1):
            print(f"   {i}. {name}: {conf:.2%}")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        plt.figure(figsize=(10, 4))
        
        plt.subplot(1, 2, 1)
        plt.imshow(img)
        plt.title(f"–¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {class_name}")
        plt.axis('off')
        
        plt.subplot(1, 2, 2)
        # –ë–∞—Ä-plot –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        classes_to_show = min(5, len(class_names))
        top_indices = np.argsort(predictions[0])[-classes_to_show:][::-1]
        top_probs = predictions[0][top_indices]
        top_labels = [class_names[i] if i < len(class_names) else f"Class {i}" for i in top_indices]
        
        plt.barh(range(classes_to_show), top_probs)
        plt.yticks(range(classes_to_show), top_labels)
        plt.xlabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
        plt.title('–¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')
        plt.gca().invert_yaxis()
        
        plt.tight_layout()
        plt.show()
>>>>>>> e4a4378eac530946868097685580eb82d315742b
        
        return class_name, confidence
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return None, 0

<<<<<<< HEAD
def evaluate_model_on_all_images(model, image_paths, true_labels, class_names, model_type="–ø–æ—Ä–æ–¥—ã"):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –≤—ã–≤–æ–¥–æ–º ‚úÖ/‚ùå –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    print(f"\nüìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ù–ê –í–°–ï–• –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–• ({model_type}):")
    print("=" * 60)
    
=======
def evaluate_model(model, image_paths, labels, class_names, model_type="–ø–æ—Ä–æ–¥—ã"):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    if len(image_paths) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
        return 0
    
<<<<<<< HEAD
    correct_predictions = 0
    total_images = len(image_paths)
    
    print(f"üî¢ –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {total_images}")
    print("-" * 60)
    
    for i, (img_path, true_label) in enumerate(zip(image_paths, true_labels)):
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img_array = load_and_preprocess_image(img_path)
            if img_array is None:
                continue
                
            img_array = np.expand_dims(img_array, axis=0)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
=======
    print(f"\nüìä –ü–û–õ–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ({model_type}):")
    print("=" * 50)
    
    correct = 0
    total = len(image_paths)
    
    for i, (img_path, true_label) in enumerate(zip(image_paths, labels)):
        try:
            img = keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)
            img_array = keras.preprocessing.image.img_to_array(img)
            img_array = np.expand_dims(img_array, axis=0) / 255.0
            
>>>>>>> e4a4378eac530946868097685580eb82d315742b
            predictions = model.predict(img_array, verbose=0)
            predicted_class = np.argmax(predictions[0])
            confidence = np.max(predictions[0])
            
<<<<<<< HEAD
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            is_correct = (predicted_class == true_label)
            if is_correct:
                correct_predictions += 1
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
            true_class_name = class_names[true_label] if true_label < len(class_names) else f"Class {true_label}"
            pred_class_name = class_names[predicted_class] if predicted_class < len(class_names) else f"Class {predicted_class}"
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å —ç–º–æ–¥–∑–∏
            status = "‚úÖ" if is_correct else "‚ùå"
            print(f"{status} {i+1:2d}/{total_images}: {Path(img_path).name:15} | "
                  f"–ò—Å—Ç–∏–Ω–∞: {true_class_name:20} | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {pred_class_name:20} | "
                  f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ {img_path}: {e}")
            continue
    
    accuracy = correct_predictions / total_images if total_images > 0 else 0
    print("-" * 60)
    print(f"üéØ –ò–¢–û–ì–û–í–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.2%} ({correct_predictions}/{total_images})")
    
    return accuracy

# –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
if __name__ == "__main__":
    print("üå≤ –ó–ê–ü–£–°–ö –£–ü–†–û–©–ï–ù–ù–û–ô –°–ò–°–¢–ï–ú–´ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –î–ï–†–ï–í–¨–ï–í")
    print("=" * 60)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    tf.config.set_soft_device_placement(True)
=======
            is_correct = (predicted_class == true_label)
            if is_correct:
                correct += 1
            
            status = "‚úÖ" if is_correct else "‚ùå"
            true_name = class_names[true_label] if true_label < len(class_names) else f"Class {true_label}"
            pred_name = class_names[predicted_class] if predicted_class < len(class_names) else f"Class {predicted_class}"
            
            print(f"{status} {i+1:2d}/{total}: {Path(img_path).name:15} | –ò—Å—Ç–∏–Ω–∞: {true_name:20} | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {pred_name:20} | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ {img_path}: {e}")
    
    accuracy = correct / total
    print(f"\nüéØ –ò–¢–û–ì–û–í–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.2%} ({correct}/{total})")
    return accuracy

def predict_new_image(model, image_path, class_names, model_type="–ø–æ—Ä–æ–¥—ã"):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if not Path(image_path).exists():
        print(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {image_path}")
        return
    
    print(f"\nüéØ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –î–õ–Ø –ù–û–í–û–ì–û –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø ({model_type}):")
    print("=" * 50)
    
    class_name, confidence = test_model(model, image_path, class_names, model_type)
    
    if confidence > 0.7:
        print(f"‚úÖ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
    elif confidence > 0.3:
        print(f"‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
    else:
        print(f"‚ùå –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
    
    return class_name, confidence

# –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
if __name__ == "__main__":
    print("üå≤ –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –î–ï–†–ï–í–¨–ï–í")
    print("=" * 60)
    
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ CSV —Ñ–∞–π–ª—ã
    print("üìù –°–û–ó–î–ê–ù–ò–ï –ü–†–ê–í–ò–õ–¨–ù–´–• CSV –§–ê–ô–õ–û–í...")
    create_proper_csv_files()
>>>>>>> e4a4378eac530946868097685580eb82d315742b
    
    porody_path = "data/–ø–æ—Ä–æ–¥—ã"
    char_path = "data/—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
    
<<<<<<< HEAD
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫
        if not Path(porody_path).exists():
            print(f"‚ùå –ü–∞–ø–∫–∞ —Å –ø–æ—Ä–æ–¥–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {porody_path}")
            porody_path = input("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –ø–æ—Ä–æ–¥–∞–º–∏: ")
        
        if not Path(char_path).exists():
            print(f"‚ùå –ü–∞–ø–∫–∞ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {char_path}")
            char_path = input("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏: ")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Ä–æ–¥
        porody_model, porody_history, species_names, porody_images, porody_labels = train_tree_species_model_simple(porody_path)
        
        print("\n" + "=" * 60)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã)
        defects_model, defects_history, defect_descriptions, defects_images, defects_labels = train_defects_model_simple(char_path)
        
        print("\n" + "=" * 60)
        print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("=" * 60)
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Ä–æ–¥
        if porody_model is not None and len(species_names) > 0 and len(porody_images) > 0:
            print("üå≥ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–†–û–î:")
            test_image = porody_images[0]
            simple_test_model(porody_model, test_image, species_names, "–ø–æ—Ä–æ–¥—ã")
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
            porody_accuracy = evaluate_model_on_all_images(
                porody_model, porody_images, porody_labels, species_names, "–ø–æ—Ä–æ–¥—ã"
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            try:
                porody_model.save('model_porody_simple.h5')
                print("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ—Ä–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_porody_simple.h5'")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Ä–æ–¥: {e}")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if defects_model is not None and len(defect_descriptions) > 0 and len(defects_images) > 0:
            print("\nüîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö:")
            test_image = defects_images[0]
            simple_test_model(defects_model, test_image, defect_descriptions, "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
            defects_accuracy = evaluate_model_on_all_images(
                defects_model, defects_images, defects_labels, defect_descriptions, "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            try:
                defects_model.save('model_defects_simple.h5')
                print("‚úÖ –ú–æ–¥–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_defects_simple.h5'")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {e}")
        else:
            print("\n‚ö†Ô∏è –ú–æ–¥–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –Ω–µ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å –¥–∞–Ω–Ω—ã–º–∏")
        
        print("\nüéâ –ü–†–û–ì–†–ê–ú–ú–ê –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!")
        
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É")





=======
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Ä–æ–¥
    porody_model, porody_history, species_names, porody_images, porody_labels = train_tree_species_model(porody_path)
    
    print("\n" + "=" * 60)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    defects_model, defects_history, defect_descriptions, defects_images, defects_labels = train_defects_model(char_path)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    if porody_history:
        plot_training_history(porody_history, '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Ä–æ–¥ –¥–µ—Ä–µ–≤—å–µ–≤')
    
    if defects_history:
        plot_training_history(defects_history, '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫')
    
    print("\n" + "=" * 60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("=" * 60)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Ä–æ–¥
    if porody_model is not None and len(species_names) > 0 and len(porody_images) > 0:
        print("üå≥ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–†–û–î:")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        test_image = porody_images[0]
        test_model(porody_model, test_image, species_names, "–ø–æ—Ä–æ–¥—ã")
        
        # –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        porody_accuracy = evaluate_model(porody_model, porody_images, porody_labels, species_names, "–ø–æ—Ä–æ–¥—ã")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        porody_model.save('model_porody.h5')
        print("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ—Ä–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_porody.h5'")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    if defects_model is not None and len(defect_descriptions) > 0 and len(defects_images) > 0:
        print("\nüîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö:")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        test_image = defects_images[0]
        test_model(defects_model, test_image, defect_descriptions, "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
        
        # –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        defects_accuracy = evaluate_model(defects_model, defects_images, defects_labels, defect_descriptions, "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        defects_model.save('model_defects.h5')
        print("‚úÖ –ú–æ–¥–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_defects.h5'")
    
    print("\nüéâ –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
>>>>>>> e4a4378eac530946868097685580eb82d315742b
