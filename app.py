from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from ultralytics import YOLO
import cv2
import numpy as np
import base64
import io
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import warnings
import os

app = Flask(__name__)
CORS(app)  # —Ä–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å —Ñ—Ä–æ–Ω—Ç–∞

# –û—Ç–∫–ª—é—á–∞–µ–º warnings
warnings.filterwarnings('ignore')

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
IMG_SIZE = (224, 224)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
detection_model = YOLO("models/detection_model.pt")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ (–¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –æ–±—É—á–µ–Ω–∏–µ–º)
def create_improved_model(num_classes):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å batch normalization –∏ –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é"""
    class ImprovedCNN(nn.Module):
        def __init__(self, num_classes):
            super(ImprovedCNN, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                nn.Conv2d(128, 256, kernel_size=3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                nn.Conv2d(256, 512, kernel_size=3, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((1, 1))
            )
            self.classifier = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.BatchNorm1d(256),
                nn.Dropout(0.3),
                nn.Linear(256, 128),
                nn.ReLU(inplace=True),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = x.view(x.size(0), -1)
            x = self.classifier(x)
            return x
    
    return ImprovedCNN(num_classes)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
def load_classification_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥ –∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    tree_model, bush_model, defects_model = None, None, None
    tree_class_names, bush_class_names, defects_class_names = [], [], []
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–µ—Ä–µ–≤—å–µ–≤
        checkpoint = torch.load('models/model_trees.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        tree_model = create_improved_model(num_classes)
        tree_model.load_state_dict(checkpoint['model_state_dict'])
        tree_model.to(DEVICE)
        tree_model.eval()
        tree_class_names = checkpoint['class_names']
        print("‚úÖ –ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫—É—Å—Ç–æ–≤
        checkpoint = torch.load('models/model_bushes.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        bush_model = create_improved_model(num_classes)
        bush_model.load_state_dict(checkpoint['model_state_dict'])
        bush_model.to(DEVICE)
        bush_model.eval()
        bush_class_names = checkpoint['class_names']
        print("‚úÖ –ú–æ–¥–µ–ª—å –∫—É—Å—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –∫—É—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤
        checkpoint = torch.load('models/model_defects.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        defects_model = create_improved_model(num_classes)
        defects_model.load_state_dict(checkpoint['model_state_dict'])
        defects_model.to(DEVICE)
        defects_model.eval()
        defects_class_names = checkpoint['class_names']
        print("‚úÖ –ú–æ–¥–µ–ª—å –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¥–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
    print("üîç –ö–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏ –¥–µ—Ä–µ–≤—å–µ–≤:", tree_class_names)
    print("üîç –ö–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏ –∫—É—Å—Ç–æ–≤:", bush_class_names)
    print("üîç –ö–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤:", defects_class_names)
    return (tree_model, bush_model, defects_model, 
            tree_class_names, bush_class_names, defects_class_names)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
(tree_model, bush_model, defects_model, 
 tree_class_names, bush_class_names, defects_class_names) = load_classification_models()

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
transform = transforms.Compose([
    transforms.Resize(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def classify_plant(plant_roi, model, class_names):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞—Å—Ç–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏"""
    if model is None:
        return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞", 0.0
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ROI –≤ PIL Image
        plant_roi_rgb = cv2.cvtColor(plant_roi, cv2.COLOR_BGR2RGB)
        plant_pil = Image.fromarray(plant_roi_rgb)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        image_tensor = transform(plant_pil).unsqueeze(0).to(DEVICE)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        with torch.no_grad():
            outputs = model(image_tensor)
            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
            confidence, predicted_class = torch.max(probabilities, 0)
        
        predicted_class = predicted_class.item()
        confidence = confidence.item()
        
        species_name = class_names[predicted_class] if predicted_class < len(class_names) else f"Class {predicted_class}"
        
        return species_name, confidence
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
        return "–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏", 0.0

def filter_small_boxes(boxes, image_shape, min_area_percent=0.001, min_side_percent=0.01):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –±–æ–∫—Å—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º –æ—Ç –ø–ª–æ—â–∞–¥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if len(boxes) == 0:
        return boxes
    
    img_height, img_width = image_shape[:2]
    total_image_area = img_width * img_height
    
    min_area = total_image_area * min_area_percent
    min_side = min(img_width, img_height) * min_side_percent
    
    filtered_boxes = []
    
    for box in boxes:
        x1, y1, x2, y2, conf, cls = box
        width = x2 - x1
        height = y2 - y1
        area = width * height
        
        if (area >= min_area and width >= min_side and height >= min_side):
            filtered_boxes.append(box)
    
    return np.array(filtered_boxes)

def advanced_merge_boxes(boxes, size_weight=0.7, conf_weight=0.3, distance_threshold=100):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–æ–∫—Å—ã —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π"""
    if len(boxes) == 0:
        return []
    
    centers = []
    sizes = []
    for box in boxes:
        x1, y1, x2, y2, conf, cls = box
        center_x = (x1 + x2) / 2
        center_y = (y1 + y2) / 2
        width = x2 - x1
        height = y2 - y1
        centers.append([center_x, center_y])
        sizes.append(width * height)
    
    centers = np.array(centers)
    sizes = np.array(sizes)
    
    n_boxes = len(boxes)
    visited = [False] * n_boxes
    clusters = []
    
    for i in range(n_boxes):
        if visited[i]:
            continue
            
        cluster = [i]
        visited[i] = True
        
        for j in range(i+1, n_boxes):
            if visited[j]:
                continue
                
            distance = np.sqrt(((centers[i] - centers[j]) ** 2).sum())
            
            if distance < distance_threshold:
                cluster.append(j)
                visited[j] = True
        
        clusters.append(cluster)
    
    merged_boxes = []
    
    for cluster in clusters:
        cluster_boxes = boxes[cluster]
        cluster_sizes = sizes[cluster]
        
        if len(cluster_boxes) == 1:
            merged_boxes.append(cluster_boxes[0])
        else:
            best_score = -1
            best_box = None
            
            for i, box in enumerate(cluster_boxes):
                x1, y1, x2, y2, conf, cls = box
                size_score = cluster_sizes[i] / max(sizes) if max(sizes) > 0 else 0
                conf_score = conf
                
                total_score = size_weight * size_score + conf_weight * conf_score
                
                if total_score > best_score:
                    best_score = total_score
                    best_box = box
            
            merged_boxes.append(best_box)
    
    return np.array(merged_boxes)

def visualize_boxes_with_classification(image, boxes, classification_results, class_names=None):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±–æ–∫—Å–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    img_display = image.copy()
    
    if class_names is None:
        class_names = {0: "Tree", 1: "Bush"}  
    
    for i, (box, result) in enumerate(zip(boxes, classification_results)):
        x1, y1, x2, y2, conf, cls = box
        class_id = int(cls)
        class_name = class_names.get(class_id, f"class_{class_id}")
        
        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤
        color = (69, 252, 3) if class_id == 0 else (207, 109, 132)
        
        # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
        cv2.rectangle(img_display, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–¥–ø–∏—Å—å - —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä –∏ –∫–ª–∞—Å—Å
        label = f"{class_name} #{i+1}"
        
        # –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ñ–æ–Ω–∞
        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
        
        # –ü–æ–∑–∏—Ü–∏—è –ø–æ–¥–ø–∏—Å–∏ –í–ù–£–¢–†–ò –±–æ–∫—Å–∞ (–≤ –ª–µ–≤–æ–º –≤–µ—Ä—Ö–Ω–µ–º —É–≥–ª—É)
        text_x = int(x1) + 5
        text_y = int(y1) + label_size[1] + 10
        
        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if text_y > img_display.shape[0]:
            text_y = int(y1) - 10
        
        # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (–≤–Ω—É—Ç—Ä–∏ –±–æ–∫—Å–∞)
        cv2.rectangle(img_display, 
                     (text_x - 2, text_y - label_size[1] - 5),
                     (text_x + label_size[0] + 2, text_y + 2),
                     color, -1)
        
        # –¢–µ–∫—Å—Ç
        cv2.putText(img_display, label, (text_x, text_y),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)
    
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –±—ã–ª–æ)
    height, width = img_display.shape[:2]
    max_display_size = 800
    if max(height, width) > max_display_size:
        scale = max_display_size / max(height, width)
        new_width = int(width * scale)
        new_height = int(height * scale)
        img_display = cv2.resize(img_display, (new_width, new_height))
    
    return img_display

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
DEFECTS_TRANSLATION = {
    "duplo": "–î—É–ø–ª–æ",
    "gnilye": "–ì–Ω–∏–ª—ã–µ —É—á–∞—Å—Ç–∫–∏", 
    "korni": "–ö–æ—Ä–Ω–∏",
    "pni": "–ü–µ–Ω—å",
    "rak": "–†–∞–∫ –¥–µ—Ä–µ–≤–∞",
    "sukhie": "–°—É—Ö–∏–µ –≤–µ—Ç–≤–∏",
    "sukhobochina": "–°—É—Ö–æ–±–æ—á–∏–Ω–∞",
    "treshchina": "–¢—Ä–µ—â–∏–Ω–∞",
    "vrediteli": "–í—Ä–µ–¥–∏—Ç–µ–ª–∏",
    "zdorovye": "–ó–¥–æ—Ä–æ–≤–æ–µ"
}

def translate_defect(defect_name):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
    if not defect_name:
        return "–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ"
    
    defect_lower = defect_name.lower().strip()
    return DEFECTS_TRANSLATION.get(defect_lower, defect_name)

def translate_defect(defect_name):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
    if not defect_name:
        return "–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ"
    
    defect_lower = defect_name.lower().strip()
    return DEFECTS_TRANSLATION.get(defect_lower, defect_name)  # –ï—Å–ª–∏ –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/upload", methods=["POST"])
def upload():
    file = request.files["file"]
    img_bytes = file.read()
    
    # —á–∏—Ç–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ –±–∞–π—Ç–æ–≤
    npimg = np.frombuffer(img_bytes, np.uint8)
    img = cv2.imdecode(npimg, cv2.IMREAD_COLOR)

    if img is None:
        return jsonify({"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"}), 400

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
    results = detection_model.predict(img, conf=0.3)
    boxes = results[0].boxes.data.cpu().numpy()

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–æ–∫—Å–æ–≤
    filtered_boxes = filter_small_boxes(boxes, img.shape, 
                                      min_area_percent=0.01, 
                                      min_side_percent=0.1)
    merged_boxes = advanced_merge_boxes(filtered_boxes, size_weight=0.8, conf_weight=0.2)
    
    classification_results = []
    table_data = []  # ‚Üê –î–û–ë–ê–í–õ–Ø–ï–ú –î–ê–ù–ù–´–ï –î–õ–Ø –¢–ê–ë–õ–ò–¶–´
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Ç–µ–Ω–∏—è
    for i, box in enumerate(merged_boxes):
        x1, y1, x2, y2, conf, detection_class = box
        
        # –í—ã—Ä–µ–∑–∞–µ–º –æ–±–ª–∞—Å—Ç—å —Å —Ä–∞—Å—Ç–µ–Ω–∏–µ–º
        padding = 5
        x1_padded = max(0, int(x1) - padding)
        y1_padded = max(0, int(y1) - padding)
        x2_padded = min(img.shape[1], int(x2) + padding)
        y2_padded = min(img.shape[0], int(y2) + padding)
        
        plant_roi = img[y1_padded:y2_padded, x1_padded:x2_padded]
        
        if plant_roi.size == 0:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–µ–∑–∞—Ç—å –æ–±–ª–∞—Å—Ç—å –¥–ª—è —Ä–∞—Å—Ç–µ–Ω–∏—è {i+1}")
            classification_results.append({
                'species': None,
                'species_confidence': 0.0,
                'defects': None,
                'defects_confidence': 0.0
            })
            continue
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Ä–æ–¥—ã
        species_name, species_confidence = "", 0.0
        plant_type = ""
        if detection_class == 0 and tree_model is not None:  # –î–µ—Ä–µ–≤–æ
            species_name, species_confidence = classify_plant(plant_roi, tree_model, tree_class_names)
            plant_type = "–î–µ—Ä–µ–≤–æ"
            print(f"üå≥ –†–∞—Å—Ç–µ–Ω–∏–µ {i+1} (–î–µ—Ä–µ–≤–æ): {species_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {species_confidence:.2%})")
        elif detection_class == 1 and bush_model is not None:  # –ö—É—Å—Ç
            species_name, species_confidence = classify_plant(plant_roi, bush_model, bush_class_names)
            plant_type = "–ö—É—Å—Ç"
            print(f"ü™¥ –†–∞—Å—Ç–µ–Ω–∏–µ {i+1} (–ö—É—Å—Ç): {species_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {species_confidence:.2%})")
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤
        defects_name, defects_confidence = "", 0.0
        if defects_model is not None:
            defects_name, defects_confidence = classify_plant(plant_roi, defects_model, defects_class_names)
            print(f"üîß –†–∞—Å—Ç–µ–Ω–∏–µ {i+1} - –î–µ—Ñ–µ–∫—Ç—ã: {defects_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {defects_confidence:.2%})")
        
        classification_results.append({
            'species': species_name,
            'species_confidence': species_confidence,
            'defects': defects_name,
            'defects_confidence': defects_confidence
        })
        
            
        # –§–û–†–ú–ò–†–£–ï–ú –î–ê–ù–ù–´–ï –î–õ–Ø –¢–ê–ë–õ–ò–¶–´
        translated_defect = translate_defect(defects_name)
        # –§–û–†–ú–ò–†–£–ï–ú –î–ê–ù–ù–´–ï –î–õ–Ø –¢–ê–ë–õ–ò–¶–´
        table_data.append({
            'id': i + 1,
            'plant_type': plant_type,  # "–¥–µ—Ä–µ–≤–æ" –∏–ª–∏ "–∫—É—Å—Ç"
            'species': species_name if species_name else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
            'species_confidence': round(species_confidence * 100, 1),  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–¥—ã –≤ %
            'status': translated_defect,  # –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–∞
            'defects_confidence': round(defects_confidence * 100, 1)   # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ %
        })
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    class_names = {0: "–î–µ—Ä–µ–≤–æ", 1: "–ö—É—Å—Ç"}
    
    if len(merged_boxes) > 0:
        final_display = visualize_boxes_with_classification(img, merged_boxes, classification_results)
    else:
        final_display = img.copy()

    # –ö–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ base64
    _, buffer = cv2.imencode(".jpg", final_display)
    encoded_img = base64.b64encode(buffer).decode("utf-8")

    # –í–û–ó–í–†–ê–©–ê–ï–ú –ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï –ò –î–ê–ù–ù–´–ï –î–õ–Ø –¢–ê–ë–õ–ò–¶–´
    return jsonify({
        "image": encoded_img,
        "table_data": table_data  # ‚Üê –î–û–ë–ê–í–õ–Ø–ï–ú –î–ê–ù–ù–´–ï –¢–ê–ë–õ–ò–¶–´
    })

if __name__ == "__main__":
    app.run(debug=True)