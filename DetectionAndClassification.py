import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import warnings
import os
from PIL import Image
import cv2
from collections import Counter
from ultralytics import YOLO



from ClassificationModel import ImprovedCNN

# –û—Ç–∫–ª—é—á–∞–µ–º warnings
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
IMG_SIZE = (224, 224)
BATCH_SIZE = 256
NUM_EPOCHS = 50
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")


def filter_small_boxes(boxes, image_shape, min_area_percent=0.001, min_side_percent=0.01):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –±–æ–∫—Å—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º –æ—Ç –ø–ª–æ—â–∞–¥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if len(boxes) == 0:
        return boxes
    
    img_height, img_width = image_shape[:2]
    total_image_area = img_width * img_height
    
    min_area = total_image_area * min_area_percent
    min_side = min(img_width, img_height) * min_side_percent
    
    filtered_boxes = []
    
    for box in boxes:
        x1, y1, x2, y2, conf, cls = box
        width = x2 - x1
        height = y2 - y1
        area = width * height
        
        if (area >= min_area and width >= min_side and height >= min_side):
            filtered_boxes.append(box)
    
    return np.array(filtered_boxes)

def advanced_merge_boxes(boxes, size_weight=0.7, conf_weight=0.3, distance_threshold=100):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–æ–∫—Å—ã —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π"""
    if len(boxes) == 0:
        return []
    
    centers = []
    sizes = []
    for box in boxes:
        x1, y1, x2, y2, conf, cls = box
        center_x = (x1 + x2) / 2
        center_y = (y1 + y2) / 2
        width = x2 - x1
        height = y2 - y1
        centers.append([center_x, center_y])
        sizes.append(width * height)
    
    centers = np.array(centers)
    sizes = np.array(sizes)
    
    n_boxes = len(boxes)
    visited = [False] * n_boxes
    clusters = []
    
    for i in range(n_boxes):
        if visited[i]:
            continue
            
        cluster = [i]
        visited[i] = True
        
        for j in range(i+1, n_boxes):
            if visited[j]:
                continue
                
            distance = np.sqrt(((centers[i] - centers[j]) ** 2).sum())
            
            if distance < distance_threshold:
                cluster.append(j)
                visited[j] = True
        
        clusters.append(cluster)
    
    merged_boxes = []
    
    for cluster in clusters:
        cluster_boxes = boxes[cluster]
        cluster_sizes = sizes[cluster]
        
        if len(cluster_boxes) == 1:
            merged_boxes.append(cluster_boxes[0])
        else:
            best_score = -1
            best_box = None
            
            for i, box in enumerate(cluster_boxes):
                x1, y1, x2, y2, conf, cls = box
                size_score = cluster_sizes[i] / max(sizes) if max(sizes) > 0 else 0
                conf_score = conf
                
                total_score = size_weight * size_score + conf_weight * conf_score
                
                if total_score > best_score:
                    best_score = total_score
                    best_box = box
            
            merged_boxes.append(best_box)
    
    return np.array(merged_boxes)

def detect_and_classify_plants(image_path, detection_model, tree_model, bush_model, 
                              tree_class_names, bush_class_names, 
                              min_confidence=0.3, min_area_percent=0.01):
    """–î–µ—Ç–µ–∫—Ü–∏—è —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø–æ —Ç–∏–ø—É —Ä–∞—Å—Ç–µ–Ω–∏—è"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = cv2.imread(image_path)
    if img is None:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        return None, []
    
    print(f"üîç –î–µ—Ç–µ–∫—Ü–∏—è —Ä–∞—Å—Ç–µ–Ω–∏–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏: {Path(image_path).name}")
    
    # –î–µ—Ç–µ–∫—Ü–∏—è —Å –ø–æ–º–æ—â—å—é YOLO
    try:
        results = detection_model.predict(img, conf=min_confidence)
        boxes = results[0].boxes.data.cpu().numpy()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
        return None, []
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–æ–∫—Å–æ–≤
    filtered_boxes = filter_small_boxes(boxes, img.shape, 
                                      min_area_percent=min_area_percent, 
                                      min_side_percent=0.1)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –±–æ–∫—Å–æ–≤
    merged_boxes = advanced_merge_boxes(filtered_boxes, size_weight=0.8, conf_weight=0.2)
    
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞—Å—Ç–µ–Ω–∏–π: {len(merged_boxes)}")
    
    if len(merged_boxes) == 0:
        print("–†–∞—Å—Ç–µ–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
        return img, []
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    transform = transforms.Compose([
        transforms.Resize(IMG_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    classification_results = []
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Ç–µ–Ω–∏—è
    for i, box in enumerate(merged_boxes):
        x1, y1, x2, y2, conf, detection_class = box
        
        # detection_class: 0 - –¥–µ—Ä–µ–≤–æ, 1 - –∫—É—Å—Ç
        plant_type = "–¥–µ—Ä–µ–≤–æ" if detection_class == 0 else "–∫—É—Å—Ç"
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –∏ –∫–ª–∞—Å—Å—ã
        if detection_class == 0 and tree_model is not None:  # –î–µ—Ä–µ–≤–æ
            classification_model = tree_model
            class_names = tree_class_names
            model_type = "–¥–µ—Ä–µ–≤—å–µ–≤"
        elif detection_class == 1 and bush_model is not None:  # –ö—É—Å—Ç
            classification_model = bush_model  
            class_names = bush_class_names
            model_type = "–∫—É—Å—Ç–æ–≤"
        else:
            print(f"–î–ª—è {plant_type} –Ω–µ—Ç –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ—Ç–µ–∫—Ü–∏–∏ –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            classification_results.append({
                'box': (int(x1), int(y1), int(x2), int(y2)),
                'plant_type': plant_type,
                'detection_confidence': conf,
                'species': "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                'classification_confidence': 0.0,
                'species_id': -1,
                'detection_class': detection_class
            })
            continue
        
        # –í—ã—Ä–µ–∑–∞–µ–º –æ–±–ª–∞—Å—Ç—å —Å —Ä–∞—Å—Ç–µ–Ω–∏–µ–º
        padding = 5
        x1_padded = max(0, int(x1) - padding)
        y1_padded = max(0, int(y1) - padding)
        x2_padded = min(img.shape[1], int(x2) + padding)
        y2_padded = min(img.shape[0], int(y2) + padding)
        
        plant_roi = img[y1_padded:y2_padded, x1_padded:x2_padded]
        
        if plant_roi.size == 0:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–µ–∑–∞—Ç—å –æ–±–ª–∞—Å—Ç—å –¥–ª—è {plant_type} {i+1}")
            continue
            
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ PIL Image –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
        try:
            plant_roi_rgb = cv2.cvtColor(plant_roi, cv2.COLOR_BGR2RGB)
            plant_pil = Image.fromarray(plant_roi_rgb)
            
            image_tensor = transform(plant_pil).unsqueeze(0).to(DEVICE)
            
            classification_model.eval()
            with torch.no_grad():
                outputs = classification_model(image_tensor)
                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                confidence, predicted_class = torch.max(probabilities, 0)
            
            predicted_class = predicted_class.item()
            confidence = confidence.item()
            
            species_name = class_names[predicted_class] if predicted_class < len(class_names) else f"Class {predicted_class}"
            
            classification_results.append({
                'box': (int(x1), int(y1), int(x2), int(y2)),
                'plant_type': plant_type,
                'detection_confidence': conf,
                'species': species_name,
                'classification_confidence': confidence,
                'species_id': predicted_class,
                'detection_class': detection_class
            })
            
            print(f"–†–∞—Å—Ç–µ–Ω–∏–µ {i+1} ({plant_type}): {species_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%})")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ {plant_type} {i+1}: {e}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ—Ç–µ–∫—Ü–∏–∏ –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            classification_results.append({
                'box': (int(x1), int(y1), int(x2), int(y2)),
                'plant_type': plant_type,
                'detection_confidence': conf,
                'species': "–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
                'classification_confidence': 0.0,
                'species_id': -1,
                'detection_class': detection_class
            })
            continue
    
    return img, classification_results

def visualize_detection_with_classification(image, boxes, classification_results):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –æ–∫–Ω–æ–º"""
    img_display = image.copy()
    
    for i, (box, result) in enumerate(zip(boxes, classification_results)):
        x1, y1, x2, y2 = result['box']
        plant_type = result['plant_type']
        species = result['species']
        det_conf = result['detection_confidence']
        cls_conf = result['classification_confidence']
        
        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤
        if result['detection_class'] == 0:  # –î–µ—Ä–µ–≤–æ
            color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π
        else:  # –ö—É—Å—Ç
            color = (0, 165, 255)  # –û—Ä–∞–Ω–∂–µ–≤—ã–π
        
        # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
        cv2.rectangle(img_display, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
        
        # –ü–æ–¥–ø–∏—Å—å —Å –≤–∏–¥–æ–º —Ä–∞—Å—Ç–µ–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        label = f"{plant_type}: {species}"
        confidence_label = f"det({det_conf:.2f}), cls({cls_conf:.2f})"
        
        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
        confidence_size = cv2.getTextSize(confidence_label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        
        total_height = label_size[1] + confidence_size[1] + 15
        
        # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        cv2.rectangle(img_display, 
                     (int(x1), int(y1) - total_height),
                     (int(x1) + max(label_size[0], confidence_size[0]), int(y1)),
                     color, -1)
        
        # –¢–µ–∫—Å—Ç
        cv2.putText(img_display, label, 
                   (int(x1), int(y1) - confidence_size[1] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.putText(img_display, confidence_label,
                   (int(x1), int(y1) - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # –ù–æ–º–µ—Ä —Ä–∞—Å—Ç–µ–Ω–∏—è
        cv2.putText(img_display, f"#{i+1}", 
                   (int(x1), int(y1) - total_height - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
    
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    height, width = img_display.shape[:2]
    max_display_size = 1200
    if max(height, width) > max_display_size:
        scale = max_display_size / max(height, width)
        new_width = int(width * scale)
        new_height = int(height * scale)
        img_display = cv2.resize(img_display, (new_width, new_height))
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∑–∞–∫—Ä—ã—Ç–∏—è
    window_name = "–î–µ—Ç–µ–∫—Ü–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞—Å—Ç–µ–Ω–∏–π (–Ω–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)"
    cv2.imshow(window_name, img_display)
    print("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞–Ω–æ. –ù–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –≤ –æ–∫–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
    
    # –ñ–¥–µ–º –Ω–∞–∂–∞—Ç–∏—è –∫–ª–∞–≤–∏—à–∏ (0 - –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ)
    cv2.waitKey(0)
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –æ–∫–Ω–∞ OpenCV
    cv2.destroyAllWindows()
    
    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –æ–∫–Ω–∞ —Ç–æ—á–Ω–æ –∑–∞–∫—Ä—ã–ª–∏—Å—å
    import time
    time.sleep(0.5)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    output_path = "detection_classification_result.jpg"
    cv2.imwrite(output_path, cv2.cvtColor(img_display, cv2.COLOR_RGB2BGR))
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {output_path}")
    
    return img_display

def load_separated_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤"""
    tree_model, bush_model = None, None
    tree_class_names, bush_class_names = [], []
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–µ—Ä–µ–≤—å–µ–≤
        checkpoint = torch.load('model_trees.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        tree_model = ImprovedCNN(num_classes)
        tree_model.load_state_dict(checkpoint['model_state_dict'])
        tree_class_names = checkpoint['class_names']
        print("–ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"–ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫—É—Å—Ç–æ–≤
        checkpoint = torch.load('model_bushes.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        bush_model = ImprovedCNN(num_classes)
        bush_model.load_state_dict(checkpoint['model_state_dict'])
        bush_class_names = checkpoint['class_names']
        print("–ú–æ–¥–µ–ª—å –∫—É—Å—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"–ú–æ–¥–µ–ª—å –∫—É—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    return tree_model, bush_model, tree_class_names, bush_class_names

def detect_and_classify_complete(image_path, detection_model, tree_model, bush_model, defects_model,
                               tree_class_names, bush_class_names, defects_class_names,
                               min_confidence=0.3, min_area_percent=0.01):
    """–ü–æ–ª–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –ø–æ—Ä–æ–¥—ã + –¥–µ—Ñ–µ–∫—Ç—ã"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = cv2.imread(image_path)
    if img is None:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        return None, []
    
    print(f"üîç –î–µ—Ç–µ–∫—Ü–∏—è –∏ –ø–æ–ª–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {Path(image_path).name}")
    
    # –î–µ—Ç–µ–∫—Ü–∏—è —Å –ø–æ–º–æ—â—å—é YOLO
    try:
        results = detection_model.predict(img, conf=min_confidence)
        boxes = results[0].boxes.data.cpu().numpy()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
        return None, []
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–æ–∫—Å–æ–≤
    filtered_boxes = filter_small_boxes(boxes, img.shape, 
                                      min_area_percent=min_area_percent, 
                                      min_side_percent=0.1)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –±–æ–∫—Å–æ–≤
    merged_boxes = advanced_merge_boxes(filtered_boxes, size_weight=0.8, conf_weight=0.2)
    
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞—Å—Ç–µ–Ω–∏–π: {len(merged_boxes)}")
    
    if len(merged_boxes) == 0:
        print("–†–∞—Å—Ç–µ–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
        return img, []
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    transform = transforms.Compose([
        transforms.Resize(IMG_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    classification_results = []
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Ç–µ–Ω–∏—è
    for i, box in enumerate(merged_boxes):
        x1, y1, x2, y2, conf, detection_class = box
        
        # detection_class: 0 - –¥–µ—Ä–µ–≤–æ, 1 - –∫—É—Å—Ç
        plant_type = "–¥–µ—Ä–µ–≤–æ" if detection_class == 0 else "–∫—É—Å—Ç"
        
        # –í–´–ë–û–† –ú–û–î–ï–õ–ò –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ü–û–†–û–î–´
        species_model = None
        species_class_names = []
        if detection_class == 0 and tree_model is not None:  # –î–µ—Ä–µ–≤–æ
            species_model = tree_model
            species_class_names = tree_class_names
        elif detection_class == 1 and bush_model is not None:  # –ö—É—Å—Ç
            species_model = bush_model  
            species_class_names = bush_class_names
        
        # –í—ã—Ä–µ–∑–∞–µ–º –æ–±–ª–∞—Å—Ç—å —Å —Ä–∞—Å—Ç–µ–Ω–∏–µ–º
        padding = 5
        x1_padded = max(0, int(x1) - padding)
        y1_padded = max(0, int(y1) - padding)
        x2_padded = min(img.shape[1], int(x2) + padding)
        y2_padded = min(img.shape[0], int(y2) + padding)
        
        plant_roi = img[y1_padded:y2_padded, x1_padded:x2_padded]
        
        if plant_roi.size == 0:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ä–µ–∑–∞—Ç—å –æ–±–ª–∞—Å—Ç—å –¥–ª—è —Ä–∞—Å—Ç–µ–Ω–∏—è {i+1}")
            continue
        
        species_name = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        species_confidence = 0.0
        defects_name = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        defects_confidence = 0.0
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–û–†–û–î–´
        if species_model is not None:
            try:
                plant_roi_rgb = cv2.cvtColor(plant_roi, cv2.COLOR_BGR2RGB)
                plant_pil = Image.fromarray(plant_roi_rgb)
                
                image_tensor = transform(plant_pil).unsqueeze(0).to(DEVICE)
                
                species_model.eval()
                with torch.no_grad():
                    outputs = species_model(image_tensor)
                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                    confidence, predicted_class = torch.max(probabilities, 0)
                
                predicted_class = predicted_class.item()
                species_confidence = confidence.item()
                
                species_name = species_class_names[predicted_class] if predicted_class < len(species_class_names) else f"Class {predicted_class}"
                
                print(f"–†–∞—Å—Ç–µ–Ω–∏–µ {i+1} ({plant_type}): {species_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {species_confidence:.2%})")
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥—ã —Ä–∞—Å—Ç–µ–Ω–∏—è {i+1}: {e}")
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –î–ï–§–ï–ö–¢–û–í
        if defects_model is not None:
            try:
                defects_model.eval()
                with torch.no_grad():
                    outputs = defects_model(image_tensor)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ç–µ–Ω–∑–æ—Ä
                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                    confidence, predicted_class = torch.max(probabilities, 0)
                
                predicted_class = predicted_class.item()
                defects_confidence = confidence.item()
                
                defects_name = defects_class_names[predicted_class] if predicted_class < len(defects_class_names) else f"Class {predicted_class}"
                
                print(f"–†–∞—Å—Ç–µ–Ω–∏–µ {i+1} - –î–µ—Ñ–µ–∫—Ç—ã: {defects_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {defects_confidence:.2%})")
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Ä–∞—Å—Ç–µ–Ω–∏—è {i+1}: {e}")
        
        classification_results.append({
            'box': (int(x1), int(y1), int(x2), int(y2)),
            'plant_type': plant_type,
            'detection_confidence': conf,
            'species': species_name,
            'species_confidence': species_confidence,
            'defects': defects_name,
            'defects_confidence': defects_confidence,
            'detection_class': detection_class,
            'plant_number': i+1  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Ä–∞—Å—Ç–µ–Ω–∏—è
        })
    
    return img, classification_results

def visualize_complete_detection(image, classification_results):
    img_display = image.copy()
    
    for result in classification_results:
        x1, y1, x2, y2 = result['box']
        species = result['species']
        defects = result['defects']
        det_conf = result['detection_confidence']
        species_conf = result['species_confidence']
        defects_conf = result['defects_confidence']
        plant_number = result['plant_number']
        
        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –∫—É—Å—Ç–æ–≤
        if result['detection_class'] == 0:  # –î–µ—Ä–µ–≤–æ
            color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π
        else:  # –ö—É—Å—Ç
            color = (0, 165, 255)  # –û—Ä–∞–Ω–∂–µ–≤—ã–π
        
        # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
        cv2.rectangle(img_display, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
        
        # –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        number_text = f"{plant_number}"
        species_text = f"–ü–æ—Ä–æ–¥–∞: {species}"
        defects_text = f"–î–µ—Ñ–µ–∫—Ç—ã: {defects}"
        confidence_text = f"–î–µ—Ç–µ–∫—Ü–∏—è: {det_conf:.2f}, –ü–æ—Ä–æ–¥–∞: {species_conf:.2f}, –î–µ—Ñ–µ–∫—Ç—ã: {defects_conf:.2f}"
        
        # –†–∞–∑–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–∞
        number_size = cv2.getTextSize(number_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
        species_size = cv2.getTextSize(species_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        defects_size = cv2.getTextSize(defects_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        confidence_size = cv2.getTextSize(confidence_text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é –≤—ã—Å–æ—Ç—É –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞
        total_height = number_size[1] + species_size[1] + defects_size[1] + confidence_size[1] + 20
        max_width = max(number_size[0], species_size[0], defects_size[0], confidence_size[0])
        
        # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ - –ë–û–õ–ï–ï –Ø–†–ö–ò–ô –ò –ß–ï–¢–ö–ò–ô
        cv2.rectangle(img_display, 
                     (int(x1), int(y1) - total_height),
                     (int(x1) + max_width + 10, int(y1)),
                     (50, 50, 50), -1)  # –¢–µ–º–Ω–æ-—Å–µ—Ä—ã–π —Ñ–æ–Ω
        
        # –†–∞–º–∫–∞ –≤–æ–∫—Ä—É–≥ —Ñ–æ–Ω–∞
        cv2.rectangle(img_display, 
                     (int(x1), int(y1) - total_height),
                     (int(x1) + max_width + 10, int(y1)),
                     color, 2)
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏ —Ü–≤–µ—Ç–∞–º–∏
        y_offset = y1 - 5
        
        # –ù–æ–º–µ—Ä —Ä–∞—Å—Ç–µ–Ω–∏—è - –ö–†–£–ü–ù–´–ô –ò –Ø–†–ö–ò–ô
        cv2.putText(img_display, number_text, 
                   (int(x1) + 5, int(y_offset)), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        y_offset -= (number_size[1] + 5)
        
        # –ü–æ—Ä–æ–¥–∞
        cv2.putText(img_display, species_text,
                   (int(x1) + 5, int(y_offset)),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        y_offset -= (species_size[1] + 5)
        
        # –î–µ—Ñ–µ–∫—Ç—ã
        cv2.putText(img_display, defects_text,
                   (int(x1) + 5, int(y_offset)),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 100), 1)  # –ñ–µ–ª—Ç—ã–π —Ü–≤–µ—Ç –¥–ª—è –¥–µ—Ñ–µ–∫—Ç–æ–≤
        y_offset -= (defects_size[1] + 5)
        
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–º–µ–ª–∫–∏–º —à—Ä–∏—Ñ—Ç–æ–º)
        cv2.putText(img_display, confidence_text,
                   (int(x1) + 5, int(y_offset)),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
    
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    height, width = img_display.shape[:2]
    max_display_size = 1200
    if max(height, width) > max_display_size:
        scale = max_display_size / max(height, width)
        new_width = int(width * scale)
        new_height = int(height * scale)
        img_display = cv2.resize(img_display, (new_width, new_height))
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    window_name = "–î–µ—Ç–µ–∫—Ü–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞—Å—Ç–µ–Ω–∏–π (–Ω–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É)"
    cv2.imshow(window_name, img_display)
    
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    
    import time
    time.sleep(0.5)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    output_path = "complete_detection_result.jpg"
    cv2.imwrite(output_path, cv2.cvtColor(img_display, cv2.COLOR_RGB2BGR))
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {output_path}")
    
    return img_display

def load_defects_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    defects_model = None
    defects_class_names = []
    
    try:
        checkpoint = torch.load('model_defects.pth', map_location=DEVICE)
        num_classes = len(checkpoint['class_names'])
        defects_model = ImprovedCNN(num_classes)
        defects_model.load_state_dict(checkpoint['model_state_dict'])
        defects_class_names = checkpoint['class_names']
        print("–ú–æ–¥–µ–ª—å –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"–ú–æ–¥–µ–ª—å –¥–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    return defects_model, defects_class_names
